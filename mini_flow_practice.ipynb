{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b934e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "#from keras.datasets import boston_housing\n",
    "from sklearn.datasets import fetch_openml\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54534301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        \n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c6ce2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "#target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44284501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 101\n",
      "Epoch: 1, Train Loss: 237.2488\n",
      "Epoch 1 - Train Loss: 237.2488 - Val Loss: 134.6227\n",
      "Epoch: 2, Train Loss: 48.1934\n",
      "Epoch 2 - Train Loss: 48.1934 - Val Loss: 93.8700\n",
      "Epoch: 3, Train Loss: 21.0880\n",
      "Epoch 3 - Train Loss: 21.0880 - Val Loss: 23.2107\n",
      "Epoch: 4, Train Loss: 19.2110\n",
      "Epoch 4 - Train Loss: 19.2110 - Val Loss: 10.3950\n",
      "Epoch: 5, Train Loss: 9.9790\n",
      "Epoch 5 - Train Loss: 9.9790 - Val Loss: 4.8041\n",
      "Epoch: 6, Train Loss: 12.5865\n",
      "Epoch 6 - Train Loss: 12.5865 - Val Loss: 7.9237\n",
      "Epoch: 7, Train Loss: 17.6066\n",
      "Epoch 7 - Train Loss: 17.6066 - Val Loss: 8.1734\n",
      "Epoch: 8, Train Loss: 13.6488\n",
      "Epoch 8 - Train Loss: 13.6488 - Val Loss: 32.2218\n",
      "Epoch: 9, Train Loss: 17.4807\n",
      "Epoch 9 - Train Loss: 17.4807 - Val Loss: 17.6792\n",
      "Epoch: 10, Train Loss: 18.1394\n",
      "Epoch 10 - Train Loss: 18.1394 - Val Loss: 8.0916\n",
      "Epoch: 11, Train Loss: 10.9462\n",
      "Epoch 11 - Train Loss: 10.9462 - Val Loss: 9.4667\n",
      "Epoch: 12, Train Loss: 10.9584\n",
      "Epoch 12 - Train Loss: 10.9584 - Val Loss: 14.2200\n",
      "Epoch: 13, Train Loss: 10.3450\n",
      "Epoch 13 - Train Loss: 10.3450 - Val Loss: 22.0260\n",
      "Epoch: 14, Train Loss: 10.7196\n",
      "Epoch 14 - Train Loss: 10.7196 - Val Loss: 6.8744\n",
      "Epoch: 15, Train Loss: 7.9464\n",
      "Epoch 15 - Train Loss: 7.9464 - Val Loss: 7.0460\n",
      "Epoch: 16, Train Loss: 3.6941\n",
      "Epoch 16 - Train Loss: 3.6941 - Val Loss: 2.5504\n",
      "Epoch: 17, Train Loss: 10.7361\n",
      "Epoch 17 - Train Loss: 10.7361 - Val Loss: 3.1067\n",
      "Epoch: 18, Train Loss: 3.8404\n",
      "Epoch 18 - Train Loss: 3.8404 - Val Loss: 4.0000\n",
      "Epoch: 19, Train Loss: 5.8027\n",
      "Epoch 19 - Train Loss: 5.8027 - Val Loss: 3.6616\n",
      "Epoch: 20, Train Loss: 6.7235\n",
      "Epoch 20 - Train Loss: 6.7235 - Val Loss: 3.2342\n",
      "Epoch: 21, Train Loss: 3.3734\n",
      "Epoch 21 - Train Loss: 3.3734 - Val Loss: 3.4889\n",
      "Epoch: 22, Train Loss: 6.2993\n",
      "Epoch 22 - Train Loss: 6.2993 - Val Loss: 6.9289\n",
      "Epoch: 23, Train Loss: 5.4272\n",
      "Epoch 23 - Train Loss: 5.4272 - Val Loss: 13.3017\n",
      "Epoch: 24, Train Loss: 3.7629\n",
      "Epoch 24 - Train Loss: 3.7629 - Val Loss: 2.6144\n",
      "Epoch: 25, Train Loss: 2.7436\n",
      "Epoch 25 - Train Loss: 2.7436 - Val Loss: 3.5541\n",
      "Epoch: 26, Train Loss: 2.4566\n",
      "Epoch 26 - Train Loss: 2.4566 - Val Loss: 1.6942\n",
      "Epoch: 27, Train Loss: 3.0669\n",
      "Epoch 27 - Train Loss: 3.0669 - Val Loss: 6.3019\n",
      "Epoch: 28, Train Loss: 3.3536\n",
      "Epoch 28 - Train Loss: 3.3536 - Val Loss: 2.6114\n",
      "Epoch: 29, Train Loss: 2.9558\n",
      "Epoch 29 - Train Loss: 2.9558 - Val Loss: 2.9704\n",
      "Epoch: 30, Train Loss: 2.2651\n",
      "Epoch 30 - Train Loss: 2.2651 - Val Loss: 1.1981\n",
      "Epoch: 31, Train Loss: 2.8474\n",
      "Epoch 31 - Train Loss: 2.8474 - Val Loss: 1.1340\n",
      "Epoch: 32, Train Loss: 1.3110\n",
      "Epoch 32 - Train Loss: 1.3110 - Val Loss: 0.4984\n",
      "Epoch: 33, Train Loss: 1.7929\n",
      "Epoch 33 - Train Loss: 1.7929 - Val Loss: 2.0975\n",
      "Epoch: 34, Train Loss: 2.1494\n",
      "Epoch 34 - Train Loss: 2.1494 - Val Loss: 1.4766\n",
      "Epoch: 35, Train Loss: 3.5665\n",
      "Epoch 35 - Train Loss: 3.5665 - Val Loss: 9.3282\n",
      "Epoch: 36, Train Loss: 1.7380\n",
      "Epoch 36 - Train Loss: 1.7380 - Val Loss: 0.6083\n",
      "Epoch: 37, Train Loss: 2.9501\n",
      "Epoch 37 - Train Loss: 2.9501 - Val Loss: 1.6153\n",
      "Epoch: 38, Train Loss: 1.3565\n",
      "Epoch 38 - Train Loss: 1.3565 - Val Loss: 0.8325\n",
      "Epoch: 39, Train Loss: 1.3327\n",
      "Epoch 39 - Train Loss: 1.3327 - Val Loss: 0.7114\n",
      "Epoch: 40, Train Loss: 2.8433\n",
      "Epoch 40 - Train Loss: 2.8433 - Val Loss: 0.8192\n",
      "Epoch: 41, Train Loss: 1.3893\n",
      "Epoch 41 - Train Loss: 1.3893 - Val Loss: 1.2889\n",
      "Epoch: 42, Train Loss: 2.6834\n",
      "Epoch 42 - Train Loss: 2.6834 - Val Loss: 1.9303\n",
      "Epoch: 43, Train Loss: 1.8195\n",
      "Epoch 43 - Train Loss: 1.8195 - Val Loss: 1.1117\n",
      "Epoch: 44, Train Loss: 1.0578\n",
      "Epoch 44 - Train Loss: 1.0578 - Val Loss: 0.7069\n",
      "Epoch: 45, Train Loss: 1.6390\n",
      "Epoch 45 - Train Loss: 1.6390 - Val Loss: 1.0452\n",
      "Epoch: 46, Train Loss: 1.4749\n",
      "Epoch 46 - Train Loss: 1.4749 - Val Loss: 0.2525\n",
      "Epoch: 47, Train Loss: 2.4829\n",
      "Epoch 47 - Train Loss: 2.4829 - Val Loss: 0.2480\n",
      "Epoch: 48, Train Loss: 1.5948\n",
      "Epoch 48 - Train Loss: 1.5948 - Val Loss: 1.2431\n",
      "Epoch: 49, Train Loss: 1.4327\n",
      "Epoch 49 - Train Loss: 1.4327 - Val Loss: 5.3795\n",
      "Epoch: 50, Train Loss: 0.8225\n",
      "Epoch 50 - Train Loss: 0.8225 - Val Loss: 0.9044\n",
      "Epoch: 51, Train Loss: 1.7966\n",
      "Epoch 51 - Train Loss: 1.7966 - Val Loss: 0.7964\n",
      "Epoch: 52, Train Loss: 0.9448\n",
      "Epoch 52 - Train Loss: 0.9448 - Val Loss: 1.0602\n",
      "Epoch: 53, Train Loss: 0.8217\n",
      "Epoch 53 - Train Loss: 0.8217 - Val Loss: 1.1751\n",
      "Epoch: 54, Train Loss: 0.6616\n",
      "Epoch 54 - Train Loss: 0.6616 - Val Loss: 0.9711\n",
      "Epoch: 55, Train Loss: 0.7082\n",
      "Epoch 55 - Train Loss: 0.7082 - Val Loss: 0.2214\n",
      "Epoch: 56, Train Loss: 1.7451\n",
      "Epoch 56 - Train Loss: 1.7451 - Val Loss: 8.6205\n",
      "Epoch: 57, Train Loss: 0.5068\n",
      "Epoch 57 - Train Loss: 0.5068 - Val Loss: 0.4167\n",
      "Epoch: 58, Train Loss: 0.7597\n",
      "Epoch 58 - Train Loss: 0.7597 - Val Loss: 0.4196\n",
      "Epoch: 59, Train Loss: 2.0282\n",
      "Epoch 59 - Train Loss: 2.0282 - Val Loss: 4.2992\n",
      "Epoch: 60, Train Loss: 0.9319\n",
      "Epoch 60 - Train Loss: 0.9319 - Val Loss: 0.2725\n",
      "Epoch: 61, Train Loss: 0.4833\n",
      "Epoch 61 - Train Loss: 0.4833 - Val Loss: 0.2835\n",
      "Epoch: 62, Train Loss: 1.4097\n",
      "Epoch 62 - Train Loss: 1.4097 - Val Loss: 0.2687\n",
      "Epoch: 63, Train Loss: 1.3152\n",
      "Epoch 63 - Train Loss: 1.3152 - Val Loss: 0.5131\n",
      "Epoch: 64, Train Loss: 1.0125\n",
      "Epoch 64 - Train Loss: 1.0125 - Val Loss: 0.4784\n",
      "Epoch: 65, Train Loss: 1.1514\n",
      "Epoch 65 - Train Loss: 1.1514 - Val Loss: 3.5331\n",
      "Epoch: 66, Train Loss: 0.5292\n",
      "Epoch 66 - Train Loss: 0.5292 - Val Loss: 0.6048\n",
      "Epoch: 67, Train Loss: 0.3885\n",
      "Epoch 67 - Train Loss: 0.3885 - Val Loss: 0.1492\n",
      "Epoch: 68, Train Loss: 1.1663\n",
      "Epoch 68 - Train Loss: 1.1663 - Val Loss: 3.5292\n",
      "Epoch: 69, Train Loss: 1.2437\n",
      "Epoch 69 - Train Loss: 1.2437 - Val Loss: 0.1761\n",
      "Epoch: 70, Train Loss: 0.4326\n",
      "Epoch 70 - Train Loss: 0.4326 - Val Loss: 0.2534\n",
      "Epoch: 71, Train Loss: 0.5255\n",
      "Epoch 71 - Train Loss: 0.5255 - Val Loss: 0.5475\n",
      "Epoch: 72, Train Loss: 1.2421\n",
      "Epoch 72 - Train Loss: 1.2421 - Val Loss: 0.8029\n",
      "Epoch: 73, Train Loss: 0.7841\n",
      "Epoch 73 - Train Loss: 0.7841 - Val Loss: 0.3575\n",
      "Epoch: 74, Train Loss: 0.7968\n",
      "Epoch 74 - Train Loss: 0.7968 - Val Loss: 3.2857\n",
      "Epoch: 75, Train Loss: 0.4312\n",
      "Epoch 75 - Train Loss: 0.4312 - Val Loss: 0.2718\n",
      "Epoch: 76, Train Loss: 0.3460\n",
      "Epoch 76 - Train Loss: 0.3460 - Val Loss: 0.1823\n",
      "Epoch: 77, Train Loss: 0.6404\n",
      "Epoch 77 - Train Loss: 0.6404 - Val Loss: 0.1946\n",
      "Epoch: 78, Train Loss: 1.0508\n",
      "Epoch 78 - Train Loss: 1.0508 - Val Loss: 0.4150\n",
      "Epoch: 79, Train Loss: 1.2676\n",
      "Epoch 79 - Train Loss: 1.2676 - Val Loss: 0.2965\n",
      "Epoch: 80, Train Loss: 0.5795\n",
      "Epoch 80 - Train Loss: 0.5795 - Val Loss: 0.3348\n",
      "Epoch: 81, Train Loss: 0.4717\n",
      "Epoch 81 - Train Loss: 0.4717 - Val Loss: 0.5380\n",
      "Epoch: 82, Train Loss: 0.6496\n",
      "Epoch 82 - Train Loss: 0.6496 - Val Loss: 0.6278\n",
      "Epoch: 83, Train Loss: 0.3307\n",
      "Epoch 83 - Train Loss: 0.3307 - Val Loss: 0.3319\n",
      "Epoch: 84, Train Loss: 0.4097\n",
      "Epoch 84 - Train Loss: 0.4097 - Val Loss: 0.4298\n",
      "Epoch: 85, Train Loss: 1.0670\n",
      "Epoch 85 - Train Loss: 1.0670 - Val Loss: 2.8563\n",
      "Epoch: 86, Train Loss: 0.3249\n",
      "Epoch 86 - Train Loss: 0.3249 - Val Loss: 0.4986\n",
      "Epoch: 87, Train Loss: 0.5912\n",
      "Epoch 87 - Train Loss: 0.5912 - Val Loss: 0.1490\n",
      "Epoch: 88, Train Loss: 0.6919\n",
      "Epoch 88 - Train Loss: 0.6919 - Val Loss: 0.4918\n",
      "Epoch: 89, Train Loss: 0.3248\n",
      "Epoch 89 - Train Loss: 0.3248 - Val Loss: 0.1572\n",
      "Epoch: 90, Train Loss: 0.3745\n",
      "Epoch 90 - Train Loss: 0.3745 - Val Loss: 0.3609\n",
      "Epoch: 91, Train Loss: 0.5926\n",
      "Epoch 91 - Train Loss: 0.5926 - Val Loss: 0.3659\n",
      "Epoch: 92, Train Loss: 0.2856\n",
      "Epoch 92 - Train Loss: 0.2856 - Val Loss: 0.1636\n",
      "Epoch: 93, Train Loss: 0.3001\n",
      "Epoch 93 - Train Loss: 0.3001 - Val Loss: 0.3059\n",
      "Epoch: 94, Train Loss: 0.6312\n",
      "Epoch 94 - Train Loss: 0.6312 - Val Loss: 0.2802\n",
      "Epoch: 95, Train Loss: 1.4541\n",
      "Epoch 95 - Train Loss: 1.4541 - Val Loss: 0.4427\n",
      "Epoch: 96, Train Loss: 0.2966\n",
      "Epoch 96 - Train Loss: 0.2966 - Val Loss: 0.4644\n",
      "Epoch: 97, Train Loss: 1.0317\n",
      "Epoch 97 - Train Loss: 1.0317 - Val Loss: 0.2912\n",
      "Epoch: 98, Train Loss: 0.4547\n",
      "Epoch 98 - Train Loss: 0.4547 - Val Loss: 0.2458\n",
      "Epoch: 99, Train Loss: 0.5870\n",
      "Epoch 99 - Train Loss: 0.5870 - Val Loss: 0.1715\n",
      "Epoch: 100, Train Loss: 0.5146\n",
      "Epoch 100 - Train Loss: 0.5146 - Val Loss: 0.0865\n",
      "Epoch: 101, Train Loss: 0.5860\n",
      "Epoch 101 - Train Loss: 0.5860 - Val Loss: 0.4131\n",
      "Epoch: 102, Train Loss: 0.3359\n",
      "Epoch 102 - Train Loss: 0.3359 - Val Loss: 0.2426\n",
      "Epoch: 103, Train Loss: 0.2185\n",
      "Epoch 103 - Train Loss: 0.2185 - Val Loss: 0.3094\n",
      "Epoch: 104, Train Loss: 0.5483\n",
      "Epoch 104 - Train Loss: 0.5483 - Val Loss: 0.1701\n",
      "Epoch: 105, Train Loss: 0.4939\n",
      "Epoch 105 - Train Loss: 0.4939 - Val Loss: 0.2381\n",
      "Epoch: 106, Train Loss: 0.4544\n",
      "Epoch 106 - Train Loss: 0.4544 - Val Loss: 0.1173\n",
      "Epoch: 107, Train Loss: 0.6764\n",
      "Epoch 107 - Train Loss: 0.6764 - Val Loss: 0.3241\n",
      "Epoch: 108, Train Loss: 0.2772\n",
      "Epoch 108 - Train Loss: 0.2772 - Val Loss: 0.2137\n",
      "Epoch: 109, Train Loss: 0.2974\n",
      "Epoch 109 - Train Loss: 0.2974 - Val Loss: 0.3214\n",
      "Epoch: 110, Train Loss: 0.2755\n",
      "Epoch 110 - Train Loss: 0.2755 - Val Loss: 0.0964\n",
      "Epoch: 111, Train Loss: 0.4317\n",
      "Epoch 111 - Train Loss: 0.4317 - Val Loss: 0.1057\n",
      "Epoch: 112, Train Loss: 0.5103\n",
      "Epoch 112 - Train Loss: 0.5103 - Val Loss: 1.9961\n",
      "Epoch: 113, Train Loss: 0.9151\n",
      "Epoch 113 - Train Loss: 0.9151 - Val Loss: 1.9679\n",
      "Epoch: 114, Train Loss: 1.0288\n",
      "Epoch 114 - Train Loss: 1.0288 - Val Loss: 0.1440\n",
      "Epoch: 115, Train Loss: 0.2768\n",
      "Epoch 115 - Train Loss: 0.2768 - Val Loss: 0.3693\n",
      "Epoch: 116, Train Loss: 0.4556\n",
      "Epoch 116 - Train Loss: 0.4556 - Val Loss: 0.4232\n",
      "Epoch: 117, Train Loss: 0.4807\n",
      "Epoch 117 - Train Loss: 0.4807 - Val Loss: 0.1932\n",
      "Epoch: 118, Train Loss: 0.2924\n",
      "Epoch 118 - Train Loss: 0.2924 - Val Loss: 0.2997\n",
      "Epoch: 119, Train Loss: 0.2275\n",
      "Epoch 119 - Train Loss: 0.2275 - Val Loss: 0.1594\n",
      "Epoch: 120, Train Loss: 0.6104\n",
      "Epoch 120 - Train Loss: 0.6104 - Val Loss: 0.3473\n",
      "Epoch: 121, Train Loss: 0.3760\n",
      "Epoch 121 - Train Loss: 0.3760 - Val Loss: 0.3230\n",
      "Epoch: 122, Train Loss: 0.2408\n",
      "Epoch 122 - Train Loss: 0.2408 - Val Loss: 0.2071\n",
      "Epoch: 123, Train Loss: 0.2010\n",
      "Epoch 123 - Train Loss: 0.2010 - Val Loss: 0.0808\n",
      "Epoch: 124, Train Loss: 0.2521\n",
      "Epoch 124 - Train Loss: 0.2521 - Val Loss: 0.3656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Train Loss: 0.5725\n",
      "Epoch 125 - Train Loss: 0.5725 - Val Loss: 0.1938\n",
      "Epoch: 126, Train Loss: 0.4157\n",
      "Epoch 126 - Train Loss: 0.4157 - Val Loss: 0.2567\n",
      "Epoch: 127, Train Loss: 0.1926\n",
      "Epoch 127 - Train Loss: 0.1926 - Val Loss: 0.2419\n",
      "Epoch: 128, Train Loss: 0.4180\n",
      "Epoch 128 - Train Loss: 0.4180 - Val Loss: 0.1845\n",
      "Epoch: 129, Train Loss: 0.5656\n",
      "Epoch 129 - Train Loss: 0.5656 - Val Loss: 0.2490\n",
      "Epoch: 130, Train Loss: 0.3395\n",
      "Epoch 130 - Train Loss: 0.3395 - Val Loss: 0.1663\n",
      "Epoch: 131, Train Loss: 0.4307\n",
      "Epoch 131 - Train Loss: 0.4307 - Val Loss: 1.6770\n",
      "Epoch: 132, Train Loss: 0.4940\n",
      "Epoch 132 - Train Loss: 0.4940 - Val Loss: 0.0639\n",
      "Epoch: 133, Train Loss: 0.2658\n",
      "Epoch 133 - Train Loss: 0.2658 - Val Loss: 0.2394\n",
      "Epoch: 134, Train Loss: 0.5445\n",
      "Epoch 134 - Train Loss: 0.5445 - Val Loss: 0.1557\n",
      "Epoch: 135, Train Loss: 0.5229\n",
      "Epoch 135 - Train Loss: 0.5229 - Val Loss: 0.1585\n",
      "Epoch: 136, Train Loss: 0.3362\n",
      "Epoch 136 - Train Loss: 0.3362 - Val Loss: 0.1425\n",
      "Epoch: 137, Train Loss: 0.7325\n",
      "Epoch 137 - Train Loss: 0.7325 - Val Loss: 1.4159\n",
      "Epoch: 138, Train Loss: 0.1837\n",
      "Epoch 138 - Train Loss: 0.1837 - Val Loss: 0.1658\n",
      "Epoch: 139, Train Loss: 0.3443\n",
      "Epoch 139 - Train Loss: 0.3443 - Val Loss: 0.2276\n",
      "Epoch: 140, Train Loss: 0.1950\n",
      "Epoch 140 - Train Loss: 0.1950 - Val Loss: 0.0893\n",
      "Epoch: 141, Train Loss: 0.1531\n",
      "Epoch 141 - Train Loss: 0.1531 - Val Loss: 0.0428\n",
      "Epoch: 142, Train Loss: 0.2271\n",
      "Epoch 142 - Train Loss: 0.2271 - Val Loss: 0.2647\n",
      "Epoch: 143, Train Loss: 0.3668\n",
      "Epoch 143 - Train Loss: 0.3668 - Val Loss: 0.1354\n",
      "Epoch: 144, Train Loss: 0.1817\n",
      "Epoch 144 - Train Loss: 0.1817 - Val Loss: 0.1477\n",
      "Epoch: 145, Train Loss: 0.3323\n",
      "Epoch 145 - Train Loss: 0.3323 - Val Loss: 0.0773\n",
      "Epoch: 146, Train Loss: 0.4527\n",
      "Epoch 146 - Train Loss: 0.4527 - Val Loss: 0.0673\n",
      "Epoch: 147, Train Loss: 0.1670\n",
      "Epoch 147 - Train Loss: 0.1670 - Val Loss: 0.1258\n",
      "Epoch: 148, Train Loss: 0.3561\n",
      "Epoch 148 - Train Loss: 0.3561 - Val Loss: 0.1297\n",
      "Epoch: 149, Train Loss: 0.4370\n",
      "Epoch 149 - Train Loss: 0.4370 - Val Loss: 1.2126\n",
      "Epoch: 150, Train Loss: 0.1885\n",
      "Epoch 150 - Train Loss: 0.1885 - Val Loss: 0.0943\n",
      "Epoch: 151, Train Loss: 0.3447\n",
      "Epoch 151 - Train Loss: 0.3447 - Val Loss: 0.1450\n",
      "Epoch: 152, Train Loss: 0.4319\n",
      "Epoch 152 - Train Loss: 0.4319 - Val Loss: 0.1211\n",
      "Epoch: 153, Train Loss: 0.3427\n",
      "Epoch 153 - Train Loss: 0.3427 - Val Loss: 0.1970\n",
      "Epoch: 154, Train Loss: 0.3164\n",
      "Epoch 154 - Train Loss: 0.3164 - Val Loss: 1.3653\n",
      "Epoch: 155, Train Loss: 0.2012\n",
      "Epoch 155 - Train Loss: 0.2012 - Val Loss: 0.2651\n",
      "Epoch: 156, Train Loss: 0.5633\n",
      "Epoch 156 - Train Loss: 0.5633 - Val Loss: 1.1817\n",
      "Epoch: 157, Train Loss: 0.1575\n",
      "Epoch 157 - Train Loss: 0.1575 - Val Loss: 0.0968\n",
      "Epoch: 158, Train Loss: 0.3981\n",
      "Epoch 158 - Train Loss: 0.3981 - Val Loss: 1.1626\n",
      "Epoch: 159, Train Loss: 0.3699\n",
      "Epoch 159 - Train Loss: 0.3699 - Val Loss: 0.0923\n",
      "Epoch: 160, Train Loss: 0.4999\n",
      "Epoch 160 - Train Loss: 0.4999 - Val Loss: 0.1960\n",
      "Epoch: 161, Train Loss: 0.1692\n",
      "Epoch 161 - Train Loss: 0.1692 - Val Loss: 0.2621\n",
      "Epoch: 162, Train Loss: 0.3679\n",
      "Epoch 162 - Train Loss: 0.3679 - Val Loss: 0.2275\n",
      "Epoch: 163, Train Loss: 0.1554\n",
      "Epoch 163 - Train Loss: 0.1554 - Val Loss: 0.0320\n",
      "Epoch: 164, Train Loss: 0.5081\n",
      "Epoch 164 - Train Loss: 0.5081 - Val Loss: 0.0935\n",
      "Epoch: 165, Train Loss: 0.2876\n",
      "Epoch 165 - Train Loss: 0.2876 - Val Loss: 0.2080\n",
      "Epoch: 166, Train Loss: 0.3570\n",
      "Epoch 166 - Train Loss: 0.3570 - Val Loss: 0.0465\n",
      "Epoch: 167, Train Loss: 0.1859\n",
      "Epoch 167 - Train Loss: 0.1859 - Val Loss: 0.0636\n",
      "Epoch: 168, Train Loss: 0.2401\n",
      "Epoch 168 - Train Loss: 0.2401 - Val Loss: 0.1075\n",
      "Epoch: 169, Train Loss: 0.2832\n",
      "Epoch 169 - Train Loss: 0.2832 - Val Loss: 0.1740\n",
      "Epoch: 170, Train Loss: 0.1790\n",
      "Epoch 170 - Train Loss: 0.1790 - Val Loss: 0.1663\n",
      "Epoch: 171, Train Loss: 0.2775\n",
      "Epoch 171 - Train Loss: 0.2775 - Val Loss: 0.1277\n",
      "Epoch: 172, Train Loss: 0.3124\n",
      "Epoch 172 - Train Loss: 0.3124 - Val Loss: 0.1455\n",
      "Epoch: 173, Train Loss: 0.2233\n",
      "Epoch 173 - Train Loss: 0.2233 - Val Loss: 0.3848\n",
      "Epoch: 174, Train Loss: 0.2486\n",
      "Epoch 174 - Train Loss: 0.2486 - Val Loss: 1.0327\n",
      "Epoch: 175, Train Loss: 0.2758\n",
      "Epoch 175 - Train Loss: 0.2758 - Val Loss: 0.1072\n",
      "Epoch: 176, Train Loss: 0.3884\n",
      "Epoch 176 - Train Loss: 0.3884 - Val Loss: 0.2485\n",
      "Epoch: 177, Train Loss: 0.2458\n",
      "Epoch 177 - Train Loss: 0.2458 - Val Loss: 0.1653\n",
      "Epoch: 178, Train Loss: 0.2016\n",
      "Epoch 178 - Train Loss: 0.2016 - Val Loss: 0.1989\n",
      "Epoch: 179, Train Loss: 0.1703\n",
      "Epoch 179 - Train Loss: 0.1703 - Val Loss: 0.1811\n",
      "Epoch: 180, Train Loss: 0.3775\n",
      "Epoch 180 - Train Loss: 0.3775 - Val Loss: 0.0705\n",
      "Epoch: 181, Train Loss: 0.3472\n",
      "Epoch 181 - Train Loss: 0.3472 - Val Loss: 0.1221\n",
      "Epoch: 182, Train Loss: 0.2105\n",
      "Epoch 182 - Train Loss: 0.2105 - Val Loss: 0.8983\n",
      "Epoch: 183, Train Loss: 0.2980\n",
      "Epoch 183 - Train Loss: 0.2980 - Val Loss: 0.4129\n",
      "Epoch: 184, Train Loss: 0.2867\n",
      "Epoch 184 - Train Loss: 0.2867 - Val Loss: 0.2014\n",
      "Epoch: 185, Train Loss: 0.3204\n",
      "Epoch 185 - Train Loss: 0.3204 - Val Loss: 0.0334\n",
      "Epoch: 186, Train Loss: 0.1139\n",
      "Epoch 186 - Train Loss: 0.1139 - Val Loss: 0.2122\n",
      "Epoch: 187, Train Loss: 0.1422\n",
      "Epoch 187 - Train Loss: 0.1422 - Val Loss: 0.2444\n",
      "Epoch: 188, Train Loss: 0.1569\n",
      "Epoch 188 - Train Loss: 0.1569 - Val Loss: 0.1263\n",
      "Epoch: 189, Train Loss: 0.2102\n",
      "Epoch 189 - Train Loss: 0.2102 - Val Loss: 0.1932\n",
      "Epoch: 190, Train Loss: 0.2534\n",
      "Epoch 190 - Train Loss: 0.2534 - Val Loss: 1.2530\n",
      "Epoch: 191, Train Loss: 0.3037\n",
      "Epoch 191 - Train Loss: 0.3037 - Val Loss: 0.2410\n",
      "Epoch: 192, Train Loss: 0.2685\n",
      "Epoch 192 - Train Loss: 0.2685 - Val Loss: 0.1759\n",
      "Epoch: 193, Train Loss: 0.4832\n",
      "Epoch 193 - Train Loss: 0.4832 - Val Loss: 0.7763\n",
      "Epoch: 194, Train Loss: 0.1544\n",
      "Epoch 194 - Train Loss: 0.1544 - Val Loss: 0.0978\n",
      "Epoch: 195, Train Loss: 0.2774\n",
      "Epoch 195 - Train Loss: 0.2774 - Val Loss: 0.1032\n",
      "Epoch: 196, Train Loss: 0.2241\n",
      "Epoch 196 - Train Loss: 0.2241 - Val Loss: 0.0978\n",
      "Epoch: 197, Train Loss: 0.2591\n",
      "Epoch 197 - Train Loss: 0.2591 - Val Loss: 0.0848\n",
      "Epoch: 198, Train Loss: 0.3901\n",
      "Epoch 198 - Train Loss: 0.3901 - Val Loss: 0.7552\n",
      "Epoch: 199, Train Loss: 0.2073\n",
      "Epoch 199 - Train Loss: 0.2073 - Val Loss: 0.6918\n",
      "Epoch: 200, Train Loss: 0.3192\n",
      "Epoch 200 - Train Loss: 0.3192 - Val Loss: 0.7650\n",
      "Epoch: 201, Train Loss: 0.1591\n",
      "Epoch 201 - Train Loss: 0.1591 - Val Loss: 0.1177\n",
      "Epoch: 202, Train Loss: 0.5047\n",
      "Epoch 202 - Train Loss: 0.5047 - Val Loss: 0.0750\n",
      "Epoch: 203, Train Loss: 0.3223\n",
      "Epoch 203 - Train Loss: 0.3223 - Val Loss: 0.2125\n",
      "Epoch: 204, Train Loss: 0.2514\n",
      "Epoch 204 - Train Loss: 0.2514 - Val Loss: 0.1366\n",
      "Epoch: 205, Train Loss: 0.1316\n",
      "Epoch 205 - Train Loss: 0.1316 - Val Loss: 0.1305\n",
      "Epoch: 206, Train Loss: 0.2479\n",
      "Epoch 206 - Train Loss: 0.2479 - Val Loss: 0.1718\n",
      "Epoch: 207, Train Loss: 0.2944\n",
      "Epoch 207 - Train Loss: 0.2944 - Val Loss: 0.0381\n",
      "Epoch: 208, Train Loss: 0.2245\n",
      "Epoch 208 - Train Loss: 0.2245 - Val Loss: 0.1483\n",
      "Epoch: 209, Train Loss: 0.1962\n",
      "Epoch 209 - Train Loss: 0.1962 - Val Loss: 0.1755\n",
      "Epoch: 210, Train Loss: 0.3511\n",
      "Epoch 210 - Train Loss: 0.3511 - Val Loss: 0.6269\n",
      "Epoch: 211, Train Loss: 0.2448\n",
      "Epoch 211 - Train Loss: 0.2448 - Val Loss: 0.5854\n",
      "Epoch: 212, Train Loss: 0.1791\n",
      "Epoch 212 - Train Loss: 0.1791 - Val Loss: 0.0862\n",
      "Epoch: 213, Train Loss: 0.1858\n",
      "Epoch 213 - Train Loss: 0.1858 - Val Loss: 0.0866\n",
      "Epoch: 214, Train Loss: 0.2748\n",
      "Epoch 214 - Train Loss: 0.2748 - Val Loss: 0.0900\n",
      "Epoch: 215, Train Loss: 0.1266\n",
      "Epoch 215 - Train Loss: 0.1266 - Val Loss: 0.0657\n",
      "Epoch: 216, Train Loss: 0.2918\n",
      "Epoch 216 - Train Loss: 0.2918 - Val Loss: 0.6544\n",
      "Epoch: 217, Train Loss: 0.3095\n",
      "Epoch 217 - Train Loss: 0.3095 - Val Loss: 0.6117\n",
      "Epoch: 218, Train Loss: 0.1204\n",
      "Epoch 218 - Train Loss: 0.1204 - Val Loss: 0.0952\n",
      "Epoch: 219, Train Loss: 0.1359\n",
      "Epoch 219 - Train Loss: 0.1359 - Val Loss: 0.1366\n",
      "Epoch: 220, Train Loss: 0.1727\n",
      "Epoch 220 - Train Loss: 0.1727 - Val Loss: 0.0537\n",
      "Epoch: 221, Train Loss: 0.2040\n",
      "Epoch 221 - Train Loss: 0.2040 - Val Loss: 0.0456\n",
      "Epoch: 222, Train Loss: 0.2067\n",
      "Epoch 222 - Train Loss: 0.2067 - Val Loss: 0.0797\n",
      "Epoch: 223, Train Loss: 0.2046\n",
      "Epoch 223 - Train Loss: 0.2046 - Val Loss: 0.1308\n",
      "Epoch: 224, Train Loss: 0.1978\n",
      "Epoch 224 - Train Loss: 0.1978 - Val Loss: 0.0307\n",
      "Epoch: 225, Train Loss: 0.2303\n",
      "Epoch 225 - Train Loss: 0.2303 - Val Loss: 0.1671\n",
      "Epoch: 226, Train Loss: 0.1489\n",
      "Epoch 226 - Train Loss: 0.1489 - Val Loss: 0.0579\n",
      "Epoch: 227, Train Loss: 0.1798\n",
      "Epoch 227 - Train Loss: 0.1798 - Val Loss: 0.0359\n",
      "Epoch: 228, Train Loss: 0.1046\n",
      "Epoch 228 - Train Loss: 0.1046 - Val Loss: 0.0759\n",
      "Epoch: 229, Train Loss: 0.2043\n",
      "Epoch 229 - Train Loss: 0.2043 - Val Loss: 0.1174\n",
      "Epoch: 230, Train Loss: 0.1550\n",
      "Epoch 230 - Train Loss: 0.1550 - Val Loss: 0.1242\n",
      "Epoch: 231, Train Loss: 0.1321\n",
      "Epoch 231 - Train Loss: 0.1321 - Val Loss: 0.1107\n",
      "Epoch: 232, Train Loss: 0.1687\n",
      "Epoch 232 - Train Loss: 0.1687 - Val Loss: 0.0735\n",
      "Epoch: 233, Train Loss: 0.2515\n",
      "Epoch 233 - Train Loss: 0.2515 - Val Loss: 0.2609\n",
      "Epoch: 234, Train Loss: 0.2683\n",
      "Epoch 234 - Train Loss: 0.2683 - Val Loss: 0.1707\n",
      "Epoch: 235, Train Loss: 0.2267\n",
      "Epoch 235 - Train Loss: 0.2267 - Val Loss: 0.1284\n",
      "Epoch: 236, Train Loss: 0.1179\n",
      "Epoch 236 - Train Loss: 0.1179 - Val Loss: 0.1397\n",
      "Epoch: 237, Train Loss: 0.0992\n",
      "Epoch 237 - Train Loss: 0.0992 - Val Loss: 0.0997\n",
      "Epoch: 238, Train Loss: 0.1941\n",
      "Epoch 238 - Train Loss: 0.1941 - Val Loss: 0.1828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239, Train Loss: 0.1769\n",
      "Epoch 239 - Train Loss: 0.1769 - Val Loss: 0.0997\n",
      "Epoch: 240, Train Loss: 0.1671\n",
      "Epoch 240 - Train Loss: 0.1671 - Val Loss: 0.1193\n",
      "Epoch: 241, Train Loss: 0.1124\n",
      "Epoch 241 - Train Loss: 0.1124 - Val Loss: 0.0773\n",
      "Epoch: 242, Train Loss: 0.2170\n",
      "Epoch 242 - Train Loss: 0.2170 - Val Loss: 0.1315\n",
      "Epoch: 243, Train Loss: 0.1691\n",
      "Epoch 243 - Train Loss: 0.1691 - Val Loss: 0.0443\n",
      "Epoch: 244, Train Loss: 0.1268\n",
      "Epoch 244 - Train Loss: 0.1268 - Val Loss: 0.1078\n",
      "Epoch: 245, Train Loss: 0.1512\n",
      "Epoch 245 - Train Loss: 0.1512 - Val Loss: 0.1102\n",
      "Epoch: 246, Train Loss: 0.1392\n",
      "Epoch 246 - Train Loss: 0.1392 - Val Loss: 0.0895\n",
      "Epoch: 247, Train Loss: 0.2177\n",
      "Epoch 247 - Train Loss: 0.2177 - Val Loss: 0.0853\n",
      "Epoch: 248, Train Loss: 0.1559\n",
      "Epoch 248 - Train Loss: 0.1559 - Val Loss: 0.1184\n",
      "Epoch: 249, Train Loss: 0.1637\n",
      "Epoch 249 - Train Loss: 0.1637 - Val Loss: 0.1692\n",
      "Epoch: 250, Train Loss: 0.2232\n",
      "Epoch 250 - Train Loss: 0.2232 - Val Loss: 0.0590\n",
      "Epoch: 251, Train Loss: 0.1344\n",
      "Epoch 251 - Train Loss: 0.1344 - Val Loss: 0.4162\n",
      "Epoch: 252, Train Loss: 0.1420\n",
      "Epoch 252 - Train Loss: 0.1420 - Val Loss: 0.1115\n",
      "Epoch: 253, Train Loss: 0.2512\n",
      "Epoch 253 - Train Loss: 0.2512 - Val Loss: 0.1019\n",
      "Epoch: 254, Train Loss: 0.1461\n",
      "Epoch 254 - Train Loss: 0.1461 - Val Loss: 0.4373\n",
      "Epoch: 255, Train Loss: 0.2157\n",
      "Epoch 255 - Train Loss: 0.2157 - Val Loss: 0.1118\n",
      "Epoch: 256, Train Loss: 0.0831\n",
      "Epoch 256 - Train Loss: 0.0831 - Val Loss: 0.0975\n",
      "Epoch: 257, Train Loss: 0.1019\n",
      "Epoch 257 - Train Loss: 0.1019 - Val Loss: 0.0565\n",
      "Epoch: 258, Train Loss: 0.2034\n",
      "Epoch 258 - Train Loss: 0.2034 - Val Loss: 0.1383\n",
      "Epoch: 259, Train Loss: 0.1300\n",
      "Epoch 259 - Train Loss: 0.1300 - Val Loss: 0.1105\n",
      "Epoch: 260, Train Loss: 0.1294\n",
      "Epoch 260 - Train Loss: 0.1294 - Val Loss: 0.0899\n",
      "Epoch: 261, Train Loss: 0.2345\n",
      "Epoch 261 - Train Loss: 0.2345 - Val Loss: 0.0562\n",
      "Epoch: 262, Train Loss: 0.1202\n",
      "Epoch 262 - Train Loss: 0.1202 - Val Loss: 0.0696\n",
      "Epoch: 263, Train Loss: 0.1497\n",
      "Epoch 263 - Train Loss: 0.1497 - Val Loss: 0.0251\n",
      "Epoch: 264, Train Loss: 0.1408\n",
      "Epoch 264 - Train Loss: 0.1408 - Val Loss: 0.0219\n",
      "Epoch: 265, Train Loss: 0.0913\n",
      "Epoch 265 - Train Loss: 0.0913 - Val Loss: 0.0832\n",
      "Epoch: 266, Train Loss: 0.2249\n",
      "Epoch 266 - Train Loss: 0.2249 - Val Loss: 0.4459\n",
      "Epoch: 267, Train Loss: 0.1789\n",
      "Epoch 267 - Train Loss: 0.1789 - Val Loss: 0.0576\n",
      "Epoch: 268, Train Loss: 0.1467\n",
      "Epoch 268 - Train Loss: 0.1467 - Val Loss: 0.4038\n",
      "Epoch: 269, Train Loss: 0.2106\n",
      "Epoch 269 - Train Loss: 0.2106 - Val Loss: 0.0755\n",
      "Epoch: 270, Train Loss: 0.2467\n",
      "Epoch 270 - Train Loss: 0.2467 - Val Loss: 0.3048\n",
      "Epoch: 271, Train Loss: 0.1353\n",
      "Epoch 271 - Train Loss: 0.1353 - Val Loss: 0.2098\n",
      "Epoch: 272, Train Loss: 0.0728\n",
      "Epoch 272 - Train Loss: 0.0728 - Val Loss: 0.1160\n",
      "Epoch: 273, Train Loss: 0.0822\n",
      "Epoch 273 - Train Loss: 0.0822 - Val Loss: 0.0935\n",
      "Epoch: 274, Train Loss: 0.1898\n",
      "Epoch 274 - Train Loss: 0.1898 - Val Loss: 0.1933\n",
      "Epoch: 275, Train Loss: 0.1500\n",
      "Epoch 275 - Train Loss: 0.1500 - Val Loss: 0.1259\n",
      "Epoch: 276, Train Loss: 0.1516\n",
      "Epoch 276 - Train Loss: 0.1516 - Val Loss: 0.2598\n",
      "Epoch: 277, Train Loss: 0.0887\n",
      "Epoch 277 - Train Loss: 0.0887 - Val Loss: 0.0398\n",
      "Epoch: 278, Train Loss: 0.2047\n",
      "Epoch 278 - Train Loss: 0.2047 - Val Loss: 0.0658\n",
      "Epoch: 279, Train Loss: 0.0710\n",
      "Epoch 279 - Train Loss: 0.0710 - Val Loss: 0.0446\n",
      "Epoch: 280, Train Loss: 0.2118\n",
      "Epoch 280 - Train Loss: 0.2118 - Val Loss: 0.1127\n",
      "Epoch: 281, Train Loss: 0.1064\n",
      "Epoch 281 - Train Loss: 0.1064 - Val Loss: 0.0364\n",
      "Epoch: 282, Train Loss: 0.1308\n",
      "Epoch 282 - Train Loss: 0.1308 - Val Loss: 0.4777\n",
      "Epoch: 283, Train Loss: 0.1089\n",
      "Epoch 283 - Train Loss: 0.1089 - Val Loss: 0.0288\n",
      "Epoch: 284, Train Loss: 0.1455\n",
      "Epoch 284 - Train Loss: 0.1455 - Val Loss: 0.1269\n",
      "Epoch: 285, Train Loss: 0.1371\n",
      "Epoch 285 - Train Loss: 0.1371 - Val Loss: 0.1700\n",
      "Epoch: 286, Train Loss: 0.1402\n",
      "Epoch 286 - Train Loss: 0.1402 - Val Loss: 0.1663\n",
      "Epoch: 287, Train Loss: 0.1772\n",
      "Epoch 287 - Train Loss: 0.1772 - Val Loss: 0.1379\n",
      "Epoch: 288, Train Loss: 0.1454\n",
      "Epoch 288 - Train Loss: 0.1454 - Val Loss: 0.0453\n",
      "Epoch: 289, Train Loss: 0.0958\n",
      "Epoch 289 - Train Loss: 0.0958 - Val Loss: 0.0276\n",
      "Epoch: 290, Train Loss: 0.1300\n",
      "Epoch 290 - Train Loss: 0.1300 - Val Loss: 0.1689\n",
      "Epoch: 291, Train Loss: 0.2006\n",
      "Epoch 291 - Train Loss: 0.2006 - Val Loss: 0.0581\n",
      "Epoch: 292, Train Loss: 0.1625\n",
      "Epoch 292 - Train Loss: 0.1625 - Val Loss: 0.0709\n",
      "Epoch: 293, Train Loss: 0.1802\n",
      "Epoch 293 - Train Loss: 0.1802 - Val Loss: 0.1731\n",
      "Epoch: 294, Train Loss: 0.1764\n",
      "Epoch 294 - Train Loss: 0.1764 - Val Loss: 0.0531\n",
      "Epoch: 295, Train Loss: 0.1603\n",
      "Epoch 295 - Train Loss: 0.1603 - Val Loss: 0.0924\n",
      "Epoch: 296, Train Loss: 0.1327\n",
      "Epoch 296 - Train Loss: 0.1327 - Val Loss: 0.0621\n",
      "Epoch: 297, Train Loss: 0.1410\n",
      "Epoch 297 - Train Loss: 0.1410 - Val Loss: 0.0203\n",
      "Epoch: 298, Train Loss: 0.1154\n",
      "Epoch 298 - Train Loss: 0.1154 - Val Loss: 0.0803\n",
      "Epoch: 299, Train Loss: 0.1145\n",
      "Epoch 299 - Train Loss: 0.1145 - Val Loss: 0.1113\n",
      "Epoch: 300, Train Loss: 0.1767\n",
      "Epoch 300 - Train Loss: 0.1767 - Val Loss: 0.0750\n",
      "Epoch: 301, Train Loss: 0.1385\n",
      "Epoch 301 - Train Loss: 0.1385 - Val Loss: 0.0377\n",
      "Epoch: 302, Train Loss: 0.1484\n",
      "Epoch 302 - Train Loss: 0.1484 - Val Loss: 0.2621\n",
      "Epoch: 303, Train Loss: 0.1745\n",
      "Epoch 303 - Train Loss: 0.1745 - Val Loss: 0.0724\n",
      "Epoch: 304, Train Loss: 0.1495\n",
      "Epoch 304 - Train Loss: 0.1495 - Val Loss: 0.0311\n",
      "Epoch: 305, Train Loss: 0.1577\n",
      "Epoch 305 - Train Loss: 0.1577 - Val Loss: 0.3874\n",
      "Epoch: 306, Train Loss: 0.1755\n",
      "Epoch 306 - Train Loss: 0.1755 - Val Loss: 0.0709\n",
      "Epoch: 307, Train Loss: 0.1671\n",
      "Epoch 307 - Train Loss: 0.1671 - Val Loss: 0.2991\n",
      "Epoch: 308, Train Loss: 0.1151\n",
      "Epoch 308 - Train Loss: 0.1151 - Val Loss: 0.0656\n",
      "Epoch: 309, Train Loss: 0.1203\n",
      "Epoch 309 - Train Loss: 0.1203 - Val Loss: 0.0835\n",
      "Epoch: 310, Train Loss: 0.1434\n",
      "Epoch 310 - Train Loss: 0.1434 - Val Loss: 0.0811\n",
      "Epoch: 311, Train Loss: 0.1993\n",
      "Epoch 311 - Train Loss: 0.1993 - Val Loss: 0.2061\n",
      "Epoch: 312, Train Loss: 0.0931\n",
      "Epoch 312 - Train Loss: 0.0931 - Val Loss: 0.0751\n",
      "Epoch: 313, Train Loss: 0.1644\n",
      "Epoch 313 - Train Loss: 0.1644 - Val Loss: 0.0871\n",
      "Epoch: 314, Train Loss: 0.1247\n",
      "Epoch 314 - Train Loss: 0.1247 - Val Loss: 0.1275\n",
      "Epoch: 315, Train Loss: 0.1071\n",
      "Epoch 315 - Train Loss: 0.1071 - Val Loss: 0.1632\n",
      "Epoch: 316, Train Loss: 0.1682\n",
      "Epoch 316 - Train Loss: 0.1682 - Val Loss: 0.0903\n",
      "Epoch: 317, Train Loss: 0.0919\n",
      "Epoch 317 - Train Loss: 0.0919 - Val Loss: 0.0404\n",
      "Epoch: 318, Train Loss: 0.1675\n",
      "Epoch 318 - Train Loss: 0.1675 - Val Loss: 0.1417\n",
      "Epoch: 319, Train Loss: 0.1188\n",
      "Epoch 319 - Train Loss: 0.1188 - Val Loss: 0.0796\n",
      "Epoch: 320, Train Loss: 0.1220\n",
      "Epoch 320 - Train Loss: 0.1220 - Val Loss: 0.1064\n",
      "Epoch: 321, Train Loss: 0.0603\n",
      "Epoch 321 - Train Loss: 0.0603 - Val Loss: 0.0321\n",
      "Epoch: 322, Train Loss: 0.1173\n",
      "Epoch 322 - Train Loss: 0.1173 - Val Loss: 0.0682\n",
      "Epoch: 323, Train Loss: 0.0965\n",
      "Epoch 323 - Train Loss: 0.0965 - Val Loss: 0.0211\n",
      "Epoch: 324, Train Loss: 0.0875\n",
      "Epoch 324 - Train Loss: 0.0875 - Val Loss: 0.0422\n",
      "Epoch: 325, Train Loss: 0.0893\n",
      "Epoch 325 - Train Loss: 0.0893 - Val Loss: 0.1679\n",
      "Epoch: 326, Train Loss: 0.1183\n",
      "Epoch 326 - Train Loss: 0.1183 - Val Loss: 0.0456\n",
      "Epoch: 327, Train Loss: 0.0889\n",
      "Epoch 327 - Train Loss: 0.0889 - Val Loss: 0.1363\n",
      "Epoch: 328, Train Loss: 0.0987\n",
      "Epoch 328 - Train Loss: 0.0987 - Val Loss: 0.0389\n",
      "Epoch: 329, Train Loss: 0.0973\n",
      "Epoch 329 - Train Loss: 0.0973 - Val Loss: 0.1521\n",
      "Epoch: 330, Train Loss: 0.0512\n",
      "Epoch 330 - Train Loss: 0.0512 - Val Loss: 0.0256\n",
      "Epoch: 331, Train Loss: 0.1358\n",
      "Epoch 331 - Train Loss: 0.1358 - Val Loss: 0.1441\n",
      "Epoch: 332, Train Loss: 0.0809\n",
      "Epoch 332 - Train Loss: 0.0809 - Val Loss: 0.1238\n",
      "Epoch: 333, Train Loss: 0.0829\n",
      "Epoch 333 - Train Loss: 0.0829 - Val Loss: 0.0271\n",
      "Epoch: 334, Train Loss: 0.1276\n",
      "Epoch 334 - Train Loss: 0.1276 - Val Loss: 0.1877\n",
      "Epoch: 335, Train Loss: 0.1104\n",
      "Epoch 335 - Train Loss: 0.1104 - Val Loss: 0.0814\n",
      "Epoch: 336, Train Loss: 0.1471\n",
      "Epoch 336 - Train Loss: 0.1471 - Val Loss: 0.1492\n",
      "Epoch: 337, Train Loss: 0.1273\n",
      "Epoch 337 - Train Loss: 0.1273 - Val Loss: 0.1294\n",
      "Epoch: 338, Train Loss: 0.1073\n",
      "Epoch 338 - Train Loss: 0.1073 - Val Loss: 0.0543\n",
      "Epoch: 339, Train Loss: 0.0742\n",
      "Epoch 339 - Train Loss: 0.0742 - Val Loss: 0.0416\n",
      "Epoch: 340, Train Loss: 0.0858\n",
      "Epoch 340 - Train Loss: 0.0858 - Val Loss: 0.1149\n",
      "Epoch: 341, Train Loss: 0.1380\n",
      "Epoch 341 - Train Loss: 0.1380 - Val Loss: 0.1428\n",
      "Epoch: 342, Train Loss: 0.1119\n",
      "Epoch 342 - Train Loss: 0.1119 - Val Loss: 0.1385\n",
      "Epoch: 343, Train Loss: 0.0655\n",
      "Epoch 343 - Train Loss: 0.0655 - Val Loss: 0.1275\n",
      "Epoch: 344, Train Loss: 0.1012\n",
      "Epoch 344 - Train Loss: 0.1012 - Val Loss: 0.1323\n",
      "Epoch: 345, Train Loss: 0.1251\n",
      "Epoch 345 - Train Loss: 0.1251 - Val Loss: 0.0634\n",
      "Epoch: 346, Train Loss: 0.0757\n",
      "Epoch 346 - Train Loss: 0.0757 - Val Loss: 0.0754\n",
      "Epoch: 347, Train Loss: 0.1077\n",
      "Epoch 347 - Train Loss: 0.1077 - Val Loss: 0.0493\n",
      "Epoch: 348, Train Loss: 0.1115\n",
      "Epoch 348 - Train Loss: 0.1115 - Val Loss: 0.1524\n",
      "Epoch: 349, Train Loss: 0.0752\n",
      "Epoch 349 - Train Loss: 0.0752 - Val Loss: 0.0279\n",
      "Epoch: 350, Train Loss: 0.1160\n",
      "Epoch 350 - Train Loss: 0.1160 - Val Loss: 0.1435\n",
      "Epoch: 351, Train Loss: 0.1103\n",
      "Epoch 351 - Train Loss: 0.1103 - Val Loss: 0.0775\n",
      "Epoch: 352, Train Loss: 0.1003\n",
      "Epoch 352 - Train Loss: 0.1003 - Val Loss: 0.2280\n",
      "Epoch: 353, Train Loss: 0.0947\n",
      "Epoch 353 - Train Loss: 0.0947 - Val Loss: 0.0703\n",
      "Epoch: 354, Train Loss: 0.1003\n",
      "Epoch 354 - Train Loss: 0.1003 - Val Loss: 0.0926\n",
      "Epoch: 355, Train Loss: 0.1143\n",
      "Epoch 355 - Train Loss: 0.1143 - Val Loss: 0.1095\n",
      "Epoch: 356, Train Loss: 0.1461\n",
      "Epoch 356 - Train Loss: 0.1461 - Val Loss: 0.1489\n",
      "Epoch: 357, Train Loss: 0.0813\n",
      "Epoch 357 - Train Loss: 0.0813 - Val Loss: 0.0556\n",
      "Epoch: 358, Train Loss: 0.1346\n",
      "Epoch 358 - Train Loss: 0.1346 - Val Loss: 0.2409\n",
      "Epoch: 359, Train Loss: 0.0691\n",
      "Epoch 359 - Train Loss: 0.0691 - Val Loss: 0.0606\n",
      "Epoch: 360, Train Loss: 0.0906\n",
      "Epoch 360 - Train Loss: 0.0906 - Val Loss: 0.0726\n",
      "Epoch: 361, Train Loss: 0.0986\n",
      "Epoch 361 - Train Loss: 0.0986 - Val Loss: 0.0617\n",
      "Epoch: 362, Train Loss: 0.0976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362 - Train Loss: 0.0976 - Val Loss: 0.0360\n",
      "Epoch: 363, Train Loss: 0.1147\n",
      "Epoch 363 - Train Loss: 0.1147 - Val Loss: 0.0468\n",
      "Epoch: 364, Train Loss: 0.1209\n",
      "Epoch 364 - Train Loss: 0.1209 - Val Loss: 0.0825\n",
      "Epoch: 365, Train Loss: 0.1010\n",
      "Epoch 365 - Train Loss: 0.1010 - Val Loss: 0.1457\n",
      "Epoch: 366, Train Loss: 0.0627\n",
      "Epoch 366 - Train Loss: 0.0627 - Val Loss: 0.0247\n",
      "Epoch: 367, Train Loss: 0.0894\n",
      "Epoch 367 - Train Loss: 0.0894 - Val Loss: 0.1047\n",
      "Epoch: 368, Train Loss: 0.0805\n",
      "Epoch 368 - Train Loss: 0.0805 - Val Loss: 0.0435\n",
      "Epoch: 369, Train Loss: 0.1131\n",
      "Epoch 369 - Train Loss: 0.1131 - Val Loss: 0.1163\n",
      "Epoch: 370, Train Loss: 0.1094\n",
      "Epoch 370 - Train Loss: 0.1094 - Val Loss: 0.0663\n",
      "Epoch: 371, Train Loss: 0.0925\n",
      "Epoch 371 - Train Loss: 0.0925 - Val Loss: 0.0562\n",
      "Epoch: 372, Train Loss: 0.0991\n",
      "Epoch 372 - Train Loss: 0.0991 - Val Loss: 0.0274\n",
      "Epoch: 373, Train Loss: 0.0665\n",
      "Epoch 373 - Train Loss: 0.0665 - Val Loss: 0.0299\n",
      "Epoch: 374, Train Loss: 0.0698\n",
      "Epoch 374 - Train Loss: 0.0698 - Val Loss: 0.0116\n",
      "Epoch: 375, Train Loss: 0.0635\n",
      "Epoch 375 - Train Loss: 0.0635 - Val Loss: 0.0549\n",
      "Epoch: 376, Train Loss: 0.1197\n",
      "Epoch 376 - Train Loss: 0.1197 - Val Loss: 0.0445\n",
      "Epoch: 377, Train Loss: 0.1093\n",
      "Epoch 377 - Train Loss: 0.1093 - Val Loss: 0.1598\n",
      "Epoch: 378, Train Loss: 0.0538\n",
      "Epoch 378 - Train Loss: 0.0538 - Val Loss: 0.0161\n",
      "Epoch: 379, Train Loss: 0.0700\n",
      "Epoch 379 - Train Loss: 0.0700 - Val Loss: 0.0558\n",
      "Epoch: 380, Train Loss: 0.0690\n",
      "Epoch 380 - Train Loss: 0.0690 - Val Loss: 0.0149\n",
      "Epoch: 381, Train Loss: 0.0675\n",
      "Epoch 381 - Train Loss: 0.0675 - Val Loss: 0.0875\n",
      "Epoch: 382, Train Loss: 0.0761\n",
      "Epoch 382 - Train Loss: 0.0761 - Val Loss: 0.0751\n",
      "Epoch: 383, Train Loss: 0.1126\n",
      "Epoch 383 - Train Loss: 0.1126 - Val Loss: 0.0416\n",
      "Epoch: 384, Train Loss: 0.1138\n",
      "Epoch 384 - Train Loss: 0.1138 - Val Loss: 0.0291\n",
      "Epoch: 385, Train Loss: 0.0911\n",
      "Epoch 385 - Train Loss: 0.0911 - Val Loss: 0.0377\n",
      "Epoch: 386, Train Loss: 0.0821\n",
      "Epoch 386 - Train Loss: 0.0821 - Val Loss: 0.0734\n",
      "Epoch: 387, Train Loss: 0.0772\n",
      "Epoch 387 - Train Loss: 0.0772 - Val Loss: 0.2012\n",
      "Epoch: 388, Train Loss: 0.1007\n",
      "Epoch 388 - Train Loss: 0.1007 - Val Loss: 0.0720\n",
      "Epoch: 389, Train Loss: 0.0733\n",
      "Epoch 389 - Train Loss: 0.0733 - Val Loss: 0.0844\n",
      "Epoch: 390, Train Loss: 0.1091\n",
      "Epoch 390 - Train Loss: 0.1091 - Val Loss: 0.0136\n",
      "Epoch: 391, Train Loss: 0.0824\n",
      "Epoch 391 - Train Loss: 0.0824 - Val Loss: 0.1332\n",
      "Epoch: 392, Train Loss: 0.1084\n",
      "Epoch 392 - Train Loss: 0.1084 - Val Loss: 0.0226\n",
      "Epoch: 393, Train Loss: 0.0954\n",
      "Epoch 393 - Train Loss: 0.0954 - Val Loss: 0.0987\n",
      "Epoch: 394, Train Loss: 0.0887\n",
      "Epoch 394 - Train Loss: 0.0887 - Val Loss: 0.1853\n",
      "Epoch: 395, Train Loss: 0.0843\n",
      "Epoch 395 - Train Loss: 0.0843 - Val Loss: 0.0858\n",
      "Epoch: 396, Train Loss: 0.1110\n",
      "Epoch 396 - Train Loss: 0.1110 - Val Loss: 0.0511\n",
      "Epoch: 397, Train Loss: 0.0966\n",
      "Epoch 397 - Train Loss: 0.0966 - Val Loss: 0.1317\n",
      "Epoch: 398, Train Loss: 0.1044\n",
      "Epoch 398 - Train Loss: 0.1044 - Val Loss: 0.0551\n",
      "Epoch: 399, Train Loss: 0.0908\n",
      "Epoch 399 - Train Loss: 0.0908 - Val Loss: 0.1463\n",
      "Epoch: 400, Train Loss: 0.0901\n",
      "Epoch 400 - Train Loss: 0.0901 - Val Loss: 0.0634\n",
      "Epoch: 401, Train Loss: 0.0943\n",
      "Epoch 401 - Train Loss: 0.0943 - Val Loss: 0.0870\n",
      "Epoch: 402, Train Loss: 0.0897\n",
      "Epoch 402 - Train Loss: 0.0897 - Val Loss: 0.1655\n",
      "Epoch: 403, Train Loss: 0.0624\n",
      "Epoch 403 - Train Loss: 0.0624 - Val Loss: 0.0575\n",
      "Epoch: 404, Train Loss: 0.0688\n",
      "Epoch 404 - Train Loss: 0.0688 - Val Loss: 0.0395\n",
      "Epoch: 405, Train Loss: 0.0990\n",
      "Epoch 405 - Train Loss: 0.0990 - Val Loss: 0.0435\n",
      "Epoch: 406, Train Loss: 0.0902\n",
      "Epoch 406 - Train Loss: 0.0902 - Val Loss: 0.0711\n",
      "Epoch: 407, Train Loss: 0.0855\n",
      "Epoch 407 - Train Loss: 0.0855 - Val Loss: 0.1195\n",
      "Epoch: 408, Train Loss: 0.1109\n",
      "Epoch 408 - Train Loss: 0.1109 - Val Loss: 0.1509\n",
      "Epoch: 409, Train Loss: 0.0739\n",
      "Epoch 409 - Train Loss: 0.0739 - Val Loss: 0.0400\n",
      "Epoch: 410, Train Loss: 0.1002\n",
      "Epoch 410 - Train Loss: 0.1002 - Val Loss: 0.0307\n",
      "Epoch: 411, Train Loss: 0.0706\n",
      "Epoch 411 - Train Loss: 0.0706 - Val Loss: 0.0549\n",
      "Epoch: 412, Train Loss: 0.0627\n",
      "Epoch 412 - Train Loss: 0.0627 - Val Loss: 0.0688\n",
      "Epoch: 413, Train Loss: 0.0783\n",
      "Epoch 413 - Train Loss: 0.0783 - Val Loss: 0.1652\n",
      "Epoch: 414, Train Loss: 0.0523\n",
      "Epoch 414 - Train Loss: 0.0523 - Val Loss: 0.0783\n",
      "Epoch: 415, Train Loss: 0.0837\n",
      "Epoch 415 - Train Loss: 0.0837 - Val Loss: 0.0464\n",
      "Epoch: 416, Train Loss: 0.0898\n",
      "Epoch 416 - Train Loss: 0.0898 - Val Loss: 0.0387\n",
      "Epoch: 417, Train Loss: 0.0853\n",
      "Epoch 417 - Train Loss: 0.0853 - Val Loss: 0.0492\n",
      "Epoch: 418, Train Loss: 0.0783\n",
      "Epoch 418 - Train Loss: 0.0783 - Val Loss: 0.1015\n",
      "Epoch: 419, Train Loss: 0.0636\n",
      "Epoch 419 - Train Loss: 0.0636 - Val Loss: 0.0359\n",
      "Epoch: 420, Train Loss: 0.0706\n",
      "Epoch 420 - Train Loss: 0.0706 - Val Loss: 0.0836\n",
      "Epoch: 421, Train Loss: 0.0676\n",
      "Epoch 421 - Train Loss: 0.0676 - Val Loss: 0.0580\n",
      "Epoch: 422, Train Loss: 0.0702\n",
      "Epoch 422 - Train Loss: 0.0702 - Val Loss: 0.0540\n",
      "Epoch: 423, Train Loss: 0.0913\n",
      "Epoch 423 - Train Loss: 0.0913 - Val Loss: 0.0267\n",
      "Epoch: 424, Train Loss: 0.0694\n",
      "Epoch 424 - Train Loss: 0.0694 - Val Loss: 0.0662\n",
      "Epoch: 425, Train Loss: 0.0864\n",
      "Epoch 425 - Train Loss: 0.0864 - Val Loss: 0.0315\n",
      "Epoch: 426, Train Loss: 0.0850\n",
      "Epoch 426 - Train Loss: 0.0850 - Val Loss: 0.0252\n",
      "Epoch: 427, Train Loss: 0.0714\n",
      "Epoch 427 - Train Loss: 0.0714 - Val Loss: 0.1215\n",
      "Epoch: 428, Train Loss: 0.0742\n",
      "Epoch 428 - Train Loss: 0.0742 - Val Loss: 0.0909\n",
      "Epoch: 429, Train Loss: 0.0813\n",
      "Epoch 429 - Train Loss: 0.0813 - Val Loss: 0.0383\n",
      "Epoch: 430, Train Loss: 0.0950\n",
      "Epoch 430 - Train Loss: 0.0950 - Val Loss: 0.0403\n",
      "Epoch: 431, Train Loss: 0.0713\n",
      "Epoch 431 - Train Loss: 0.0713 - Val Loss: 0.0558\n",
      "Epoch: 432, Train Loss: 0.0926\n",
      "Epoch 432 - Train Loss: 0.0926 - Val Loss: 0.1664\n",
      "Epoch: 433, Train Loss: 0.1047\n",
      "Epoch 433 - Train Loss: 0.1047 - Val Loss: 0.1540\n",
      "Epoch: 434, Train Loss: 0.0592\n",
      "Epoch 434 - Train Loss: 0.0592 - Val Loss: 0.0777\n",
      "Epoch: 435, Train Loss: 0.0895\n",
      "Epoch 435 - Train Loss: 0.0895 - Val Loss: 0.1295\n",
      "Epoch: 436, Train Loss: 0.0774\n",
      "Epoch 436 - Train Loss: 0.0774 - Val Loss: 0.0839\n",
      "Epoch: 437, Train Loss: 0.0700\n",
      "Epoch 437 - Train Loss: 0.0700 - Val Loss: 0.0655\n",
      "Epoch: 438, Train Loss: 0.0830\n",
      "Epoch 438 - Train Loss: 0.0830 - Val Loss: 0.1736\n",
      "Epoch: 439, Train Loss: 0.0806\n",
      "Epoch 439 - Train Loss: 0.0806 - Val Loss: 0.1251\n",
      "Epoch: 440, Train Loss: 0.0726\n",
      "Epoch 440 - Train Loss: 0.0726 - Val Loss: 0.0725\n",
      "Epoch: 441, Train Loss: 0.0821\n",
      "Epoch 441 - Train Loss: 0.0821 - Val Loss: 0.0378\n",
      "Epoch: 442, Train Loss: 0.0921\n",
      "Epoch 442 - Train Loss: 0.0921 - Val Loss: 0.0736\n",
      "Epoch: 443, Train Loss: 0.0729\n",
      "Epoch 443 - Train Loss: 0.0729 - Val Loss: 0.0115\n",
      "Epoch: 444, Train Loss: 0.1043\n",
      "Epoch 444 - Train Loss: 0.1043 - Val Loss: 0.1108\n",
      "Epoch: 445, Train Loss: 0.0654\n",
      "Epoch 445 - Train Loss: 0.0654 - Val Loss: 0.0092\n",
      "Epoch: 446, Train Loss: 0.0638\n",
      "Epoch 446 - Train Loss: 0.0638 - Val Loss: 0.1140\n",
      "Epoch: 447, Train Loss: 0.0801\n",
      "Epoch 447 - Train Loss: 0.0801 - Val Loss: 0.0503\n",
      "Epoch: 448, Train Loss: 0.0798\n",
      "Epoch 448 - Train Loss: 0.0798 - Val Loss: 0.0634\n",
      "Epoch: 449, Train Loss: 0.0729\n",
      "Epoch 449 - Train Loss: 0.0729 - Val Loss: 0.0785\n",
      "Epoch: 450, Train Loss: 0.0511\n",
      "Epoch 450 - Train Loss: 0.0511 - Val Loss: 0.0183\n",
      "Epoch: 451, Train Loss: 0.0620\n",
      "Epoch 451 - Train Loss: 0.0620 - Val Loss: 0.1396\n",
      "Epoch: 452, Train Loss: 0.0714\n",
      "Epoch 452 - Train Loss: 0.0714 - Val Loss: 0.0295\n",
      "Epoch: 453, Train Loss: 0.0578\n",
      "Epoch 453 - Train Loss: 0.0578 - Val Loss: 0.1545\n",
      "Epoch: 454, Train Loss: 0.0748\n",
      "Epoch 454 - Train Loss: 0.0748 - Val Loss: 0.0757\n",
      "Epoch: 455, Train Loss: 0.0532\n",
      "Epoch 455 - Train Loss: 0.0532 - Val Loss: 0.0128\n",
      "Epoch: 456, Train Loss: 0.0632\n",
      "Epoch 456 - Train Loss: 0.0632 - Val Loss: 0.0250\n",
      "Epoch: 457, Train Loss: 0.0837\n",
      "Epoch 457 - Train Loss: 0.0837 - Val Loss: 0.1395\n",
      "Epoch: 458, Train Loss: 0.0832\n",
      "Epoch 458 - Train Loss: 0.0832 - Val Loss: 0.1173\n",
      "Epoch: 459, Train Loss: 0.0769\n",
      "Epoch 459 - Train Loss: 0.0769 - Val Loss: 0.0690\n",
      "Epoch: 460, Train Loss: 0.0657\n",
      "Epoch 460 - Train Loss: 0.0657 - Val Loss: 0.0199\n",
      "Epoch: 461, Train Loss: 0.0774\n",
      "Epoch 461 - Train Loss: 0.0774 - Val Loss: 0.0462\n",
      "Epoch: 462, Train Loss: 0.0516\n",
      "Epoch 462 - Train Loss: 0.0516 - Val Loss: 0.0179\n",
      "Epoch: 463, Train Loss: 0.0819\n",
      "Epoch 463 - Train Loss: 0.0819 - Val Loss: 0.1522\n",
      "Epoch: 464, Train Loss: 0.0696\n",
      "Epoch 464 - Train Loss: 0.0696 - Val Loss: 0.0391\n",
      "Epoch: 465, Train Loss: 0.0641\n",
      "Epoch 465 - Train Loss: 0.0641 - Val Loss: 0.0330\n",
      "Epoch: 466, Train Loss: 0.0765\n",
      "Epoch 466 - Train Loss: 0.0765 - Val Loss: 0.0655\n",
      "Epoch: 467, Train Loss: 0.0699\n",
      "Epoch 467 - Train Loss: 0.0699 - Val Loss: 0.0827\n",
      "Epoch: 468, Train Loss: 0.0917\n",
      "Epoch 468 - Train Loss: 0.0917 - Val Loss: 0.0399\n",
      "Epoch: 469, Train Loss: 0.0613\n",
      "Epoch 469 - Train Loss: 0.0613 - Val Loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 470, Train Loss: 0.0734\n",
      "Epoch 470 - Train Loss: 0.0734 - Val Loss: 0.0752\n",
      "Epoch: 471, Train Loss: 0.0736\n",
      "Epoch 471 - Train Loss: 0.0736 - Val Loss: 0.0593\n",
      "Epoch: 472, Train Loss: 0.0550\n",
      "Epoch 472 - Train Loss: 0.0550 - Val Loss: 0.0249\n",
      "Epoch: 473, Train Loss: 0.0591\n",
      "Epoch 473 - Train Loss: 0.0591 - Val Loss: 0.0511\n",
      "Epoch: 474, Train Loss: 0.0731\n",
      "Epoch 474 - Train Loss: 0.0731 - Val Loss: 0.0508\n",
      "Epoch: 475, Train Loss: 0.0740\n",
      "Epoch 475 - Train Loss: 0.0740 - Val Loss: 0.0359\n",
      "Epoch: 476, Train Loss: 0.0702\n",
      "Epoch 476 - Train Loss: 0.0702 - Val Loss: 0.0385\n",
      "Epoch: 477, Train Loss: 0.0399\n",
      "Epoch 477 - Train Loss: 0.0399 - Val Loss: 0.0262\n",
      "Epoch: 478, Train Loss: 0.0474\n",
      "Epoch 478 - Train Loss: 0.0474 - Val Loss: 0.0349\n",
      "Epoch: 479, Train Loss: 0.0533\n",
      "Epoch 479 - Train Loss: 0.0533 - Val Loss: 0.0469\n",
      "Epoch: 480, Train Loss: 0.0686\n",
      "Epoch 480 - Train Loss: 0.0686 - Val Loss: 0.0332\n",
      "Epoch: 481, Train Loss: 0.0739\n",
      "Epoch 481 - Train Loss: 0.0739 - Val Loss: 0.1164\n",
      "Epoch: 482, Train Loss: 0.0575\n",
      "Epoch 482 - Train Loss: 0.0575 - Val Loss: 0.0326\n",
      "Epoch: 483, Train Loss: 0.0604\n",
      "Epoch 483 - Train Loss: 0.0604 - Val Loss: 0.0783\n",
      "Epoch: 484, Train Loss: 0.0600\n",
      "Epoch 484 - Train Loss: 0.0600 - Val Loss: 0.0542\n",
      "Epoch: 485, Train Loss: 0.0502\n",
      "Epoch 485 - Train Loss: 0.0502 - Val Loss: 0.0783\n",
      "Epoch: 486, Train Loss: 0.0627\n",
      "Epoch 486 - Train Loss: 0.0627 - Val Loss: 0.0883\n",
      "Epoch: 487, Train Loss: 0.0783\n",
      "Epoch 487 - Train Loss: 0.0783 - Val Loss: 0.1769\n",
      "Epoch: 488, Train Loss: 0.0627\n",
      "Epoch 488 - Train Loss: 0.0627 - Val Loss: 0.0553\n",
      "Epoch: 489, Train Loss: 0.0561\n",
      "Epoch 489 - Train Loss: 0.0561 - Val Loss: 0.1077\n",
      "Epoch: 490, Train Loss: 0.0885\n",
      "Epoch 490 - Train Loss: 0.0885 - Val Loss: 0.1081\n",
      "Epoch: 491, Train Loss: 0.0834\n",
      "Epoch 491 - Train Loss: 0.0834 - Val Loss: 0.0439\n",
      "Epoch: 492, Train Loss: 0.0679\n",
      "Epoch 492 - Train Loss: 0.0679 - Val Loss: 0.0477\n",
      "Epoch: 493, Train Loss: 0.0759\n",
      "Epoch 493 - Train Loss: 0.0759 - Val Loss: 0.0462\n",
      "Epoch: 494, Train Loss: 0.0535\n",
      "Epoch 494 - Train Loss: 0.0535 - Val Loss: 0.0503\n",
      "Epoch: 495, Train Loss: 0.0585\n",
      "Epoch 495 - Train Loss: 0.0585 - Val Loss: 0.0333\n",
      "Epoch: 496, Train Loss: 0.0538\n",
      "Epoch 496 - Train Loss: 0.0538 - Val Loss: 0.0721\n",
      "Epoch: 497, Train Loss: 0.0739\n",
      "Epoch 497 - Train Loss: 0.0739 - Val Loss: 0.0560\n",
      "Epoch: 498, Train Loss: 0.0540\n",
      "Epoch 498 - Train Loss: 0.0540 - Val Loss: 0.0336\n",
      "Epoch: 499, Train Loss: 0.0600\n",
      "Epoch 499 - Train Loss: 0.0600 - Val Loss: 0.0209\n",
      "Epoch: 500, Train Loss: 0.0674\n",
      "Epoch 500 - Train Loss: 0.0674 - Val Loss: 0.0354\n",
      "Epoch: 501, Train Loss: 0.0624\n",
      "Epoch 501 - Train Loss: 0.0624 - Val Loss: 0.0571\n",
      "Epoch: 502, Train Loss: 0.0487\n",
      "Epoch 502 - Train Loss: 0.0487 - Val Loss: 0.0879\n",
      "Epoch: 503, Train Loss: 0.0616\n",
      "Epoch 503 - Train Loss: 0.0616 - Val Loss: 0.0458\n",
      "Epoch: 504, Train Loss: 0.0468\n",
      "Epoch 504 - Train Loss: 0.0468 - Val Loss: 0.0160\n",
      "Epoch: 505, Train Loss: 0.0660\n",
      "Epoch 505 - Train Loss: 0.0660 - Val Loss: 0.1273\n",
      "Epoch: 506, Train Loss: 0.0706\n",
      "Epoch 506 - Train Loss: 0.0706 - Val Loss: 0.0319\n",
      "Epoch: 507, Train Loss: 0.0513\n",
      "Epoch 507 - Train Loss: 0.0513 - Val Loss: 0.0308\n",
      "Epoch: 508, Train Loss: 0.0498\n",
      "Epoch 508 - Train Loss: 0.0498 - Val Loss: 0.0834\n",
      "Epoch: 509, Train Loss: 0.0657\n",
      "Epoch 509 - Train Loss: 0.0657 - Val Loss: 0.0399\n",
      "Epoch: 510, Train Loss: 0.0539\n",
      "Epoch 510 - Train Loss: 0.0539 - Val Loss: 0.0278\n",
      "Epoch: 511, Train Loss: 0.0593\n",
      "Epoch 511 - Train Loss: 0.0593 - Val Loss: 0.1222\n",
      "Epoch: 512, Train Loss: 0.0513\n",
      "Epoch 512 - Train Loss: 0.0513 - Val Loss: 0.0243\n",
      "Epoch: 513, Train Loss: 0.0792\n",
      "Epoch 513 - Train Loss: 0.0792 - Val Loss: 0.0656\n",
      "Epoch: 514, Train Loss: 0.0538\n",
      "Epoch 514 - Train Loss: 0.0538 - Val Loss: 0.0458\n",
      "Epoch: 515, Train Loss: 0.0647\n",
      "Epoch 515 - Train Loss: 0.0647 - Val Loss: 0.0433\n",
      "Epoch: 516, Train Loss: 0.0511\n",
      "Epoch 516 - Train Loss: 0.0511 - Val Loss: 0.0416\n",
      "Epoch: 517, Train Loss: 0.0626\n",
      "Epoch 517 - Train Loss: 0.0626 - Val Loss: 0.0479\n",
      "Epoch: 518, Train Loss: 0.0660\n",
      "Epoch 518 - Train Loss: 0.0660 - Val Loss: 0.0179\n",
      "Epoch: 519, Train Loss: 0.0490\n",
      "Epoch 519 - Train Loss: 0.0490 - Val Loss: 0.0893\n",
      "Epoch: 520, Train Loss: 0.0482\n",
      "Epoch 520 - Train Loss: 0.0482 - Val Loss: 0.0115\n",
      "Epoch: 521, Train Loss: 0.0387\n",
      "Epoch 521 - Train Loss: 0.0387 - Val Loss: 0.0470\n",
      "Epoch: 522, Train Loss: 0.0551\n",
      "Epoch 522 - Train Loss: 0.0551 - Val Loss: 0.0331\n",
      "Epoch: 523, Train Loss: 0.0448\n",
      "Epoch 523 - Train Loss: 0.0448 - Val Loss: 0.0488\n",
      "Epoch: 524, Train Loss: 0.0623\n",
      "Epoch 524 - Train Loss: 0.0623 - Val Loss: 0.0681\n",
      "Epoch: 525, Train Loss: 0.0470\n",
      "Epoch 525 - Train Loss: 0.0470 - Val Loss: 0.0476\n",
      "Epoch: 526, Train Loss: 0.0597\n",
      "Epoch 526 - Train Loss: 0.0597 - Val Loss: 0.0526\n",
      "Epoch: 527, Train Loss: 0.0642\n",
      "Epoch 527 - Train Loss: 0.0642 - Val Loss: 0.0556\n",
      "Epoch: 528, Train Loss: 0.0418\n",
      "Epoch 528 - Train Loss: 0.0418 - Val Loss: 0.0648\n",
      "Epoch: 529, Train Loss: 0.0658\n",
      "Epoch 529 - Train Loss: 0.0658 - Val Loss: 0.0799\n",
      "Epoch: 530, Train Loss: 0.0488\n",
      "Epoch 530 - Train Loss: 0.0488 - Val Loss: 0.0610\n",
      "Epoch: 531, Train Loss: 0.0502\n",
      "Epoch 531 - Train Loss: 0.0502 - Val Loss: 0.0284\n",
      "Epoch: 532, Train Loss: 0.0609\n",
      "Epoch 532 - Train Loss: 0.0609 - Val Loss: 0.0839\n",
      "Epoch: 533, Train Loss: 0.0576\n",
      "Epoch 533 - Train Loss: 0.0576 - Val Loss: 0.0118\n",
      "Epoch: 534, Train Loss: 0.0732\n",
      "Epoch 534 - Train Loss: 0.0732 - Val Loss: 0.0805\n",
      "Epoch: 535, Train Loss: 0.0572\n",
      "Epoch 535 - Train Loss: 0.0572 - Val Loss: 0.0417\n",
      "Epoch: 536, Train Loss: 0.0680\n",
      "Epoch 536 - Train Loss: 0.0680 - Val Loss: 0.0247\n",
      "Epoch: 537, Train Loss: 0.0653\n",
      "Epoch 537 - Train Loss: 0.0653 - Val Loss: 0.0141\n",
      "Epoch: 538, Train Loss: 0.0581\n",
      "Epoch 538 - Train Loss: 0.0581 - Val Loss: 0.0545\n",
      "Epoch: 539, Train Loss: 0.0724\n",
      "Epoch 539 - Train Loss: 0.0724 - Val Loss: 0.0869\n",
      "Epoch: 540, Train Loss: 0.0585\n",
      "Epoch 540 - Train Loss: 0.0585 - Val Loss: 0.0478\n",
      "Epoch: 541, Train Loss: 0.0500\n",
      "Epoch 541 - Train Loss: 0.0500 - Val Loss: 0.0286\n",
      "Epoch: 542, Train Loss: 0.0527\n",
      "Epoch 542 - Train Loss: 0.0527 - Val Loss: 0.0532\n",
      "Epoch: 543, Train Loss: 0.0531\n",
      "Epoch 543 - Train Loss: 0.0531 - Val Loss: 0.0274\n",
      "Epoch: 544, Train Loss: 0.0418\n",
      "Epoch 544 - Train Loss: 0.0418 - Val Loss: 0.0567\n",
      "Epoch: 545, Train Loss: 0.0663\n",
      "Epoch 545 - Train Loss: 0.0663 - Val Loss: 0.0190\n",
      "Epoch: 546, Train Loss: 0.0458\n",
      "Epoch 546 - Train Loss: 0.0458 - Val Loss: 0.0573\n",
      "Epoch: 547, Train Loss: 0.0503\n",
      "Epoch 547 - Train Loss: 0.0503 - Val Loss: 0.0140\n",
      "Epoch: 548, Train Loss: 0.0516\n",
      "Epoch 548 - Train Loss: 0.0516 - Val Loss: 0.0466\n",
      "Epoch: 549, Train Loss: 0.0644\n",
      "Epoch 549 - Train Loss: 0.0644 - Val Loss: 0.0734\n",
      "Epoch: 550, Train Loss: 0.0456\n",
      "Epoch 550 - Train Loss: 0.0456 - Val Loss: 0.0576\n",
      "Epoch: 551, Train Loss: 0.0543\n",
      "Epoch 551 - Train Loss: 0.0543 - Val Loss: 0.0728\n",
      "Epoch: 552, Train Loss: 0.0416\n",
      "Epoch 552 - Train Loss: 0.0416 - Val Loss: 0.0323\n",
      "Epoch: 553, Train Loss: 0.0635\n",
      "Epoch 553 - Train Loss: 0.0635 - Val Loss: 0.0786\n",
      "Epoch: 554, Train Loss: 0.0314\n",
      "Epoch 554 - Train Loss: 0.0314 - Val Loss: 0.0245\n",
      "Epoch: 555, Train Loss: 0.0433\n",
      "Epoch 555 - Train Loss: 0.0433 - Val Loss: 0.0532\n",
      "Epoch: 556, Train Loss: 0.0457\n",
      "Epoch 556 - Train Loss: 0.0457 - Val Loss: 0.0397\n",
      "Epoch: 557, Train Loss: 0.0457\n",
      "Epoch 557 - Train Loss: 0.0457 - Val Loss: 0.0123\n",
      "Epoch: 558, Train Loss: 0.0443\n",
      "Epoch 558 - Train Loss: 0.0443 - Val Loss: 0.0224\n",
      "Epoch: 559, Train Loss: 0.0523\n",
      "Epoch 559 - Train Loss: 0.0523 - Val Loss: 0.0450\n",
      "Epoch: 560, Train Loss: 0.0590\n",
      "Epoch 560 - Train Loss: 0.0590 - Val Loss: 0.0467\n",
      "Epoch: 561, Train Loss: 0.0386\n",
      "Epoch 561 - Train Loss: 0.0386 - Val Loss: 0.0308\n",
      "Epoch: 562, Train Loss: 0.0567\n",
      "Epoch 562 - Train Loss: 0.0567 - Val Loss: 0.0469\n",
      "Epoch: 563, Train Loss: 0.0459\n",
      "Epoch 563 - Train Loss: 0.0459 - Val Loss: 0.0153\n",
      "Epoch: 564, Train Loss: 0.0419\n",
      "Epoch 564 - Train Loss: 0.0419 - Val Loss: 0.0164\n",
      "Epoch: 565, Train Loss: 0.0576\n",
      "Epoch 565 - Train Loss: 0.0576 - Val Loss: 0.0395\n",
      "Epoch: 566, Train Loss: 0.0587\n",
      "Epoch 566 - Train Loss: 0.0587 - Val Loss: 0.1067\n",
      "Epoch: 567, Train Loss: 0.0509\n",
      "Epoch 567 - Train Loss: 0.0509 - Val Loss: 0.0526\n",
      "Epoch: 568, Train Loss: 0.0475\n",
      "Epoch 568 - Train Loss: 0.0475 - Val Loss: 0.0413\n",
      "Epoch: 569, Train Loss: 0.0419\n",
      "Epoch 569 - Train Loss: 0.0419 - Val Loss: 0.0460\n",
      "Epoch: 570, Train Loss: 0.0418\n",
      "Epoch 570 - Train Loss: 0.0418 - Val Loss: 0.0343\n",
      "Epoch: 571, Train Loss: 0.0537\n",
      "Epoch 571 - Train Loss: 0.0537 - Val Loss: 0.0204\n",
      "Epoch: 572, Train Loss: 0.0632\n",
      "Epoch 572 - Train Loss: 0.0632 - Val Loss: 0.0181\n",
      "Epoch: 573, Train Loss: 0.0514\n",
      "Epoch 573 - Train Loss: 0.0514 - Val Loss: 0.0379\n",
      "Epoch: 574, Train Loss: 0.0416\n",
      "Epoch 574 - Train Loss: 0.0416 - Val Loss: 0.0406\n",
      "Epoch: 575, Train Loss: 0.0794\n",
      "Epoch 575 - Train Loss: 0.0794 - Val Loss: 0.0744\n",
      "Epoch: 576, Train Loss: 0.0545\n",
      "Epoch 576 - Train Loss: 0.0545 - Val Loss: 0.0463\n",
      "Epoch: 577, Train Loss: 0.0541\n",
      "Epoch 577 - Train Loss: 0.0541 - Val Loss: 0.0181\n",
      "Epoch: 578, Train Loss: 0.0487\n",
      "Epoch 578 - Train Loss: 0.0487 - Val Loss: 0.0445\n",
      "Epoch: 579, Train Loss: 0.0517\n",
      "Epoch 579 - Train Loss: 0.0517 - Val Loss: 0.0208\n",
      "Epoch: 580, Train Loss: 0.0546\n",
      "Epoch 580 - Train Loss: 0.0546 - Val Loss: 0.0833\n",
      "Epoch: 581, Train Loss: 0.0558\n",
      "Epoch 581 - Train Loss: 0.0558 - Val Loss: 0.0170\n",
      "Epoch: 582, Train Loss: 0.0489\n",
      "Epoch 582 - Train Loss: 0.0489 - Val Loss: 0.0427\n",
      "Epoch: 583, Train Loss: 0.0532\n",
      "Epoch 583 - Train Loss: 0.0532 - Val Loss: 0.0717\n",
      "Epoch: 584, Train Loss: 0.0487\n",
      "Epoch 584 - Train Loss: 0.0487 - Val Loss: 0.0183\n",
      "Epoch: 585, Train Loss: 0.0626\n",
      "Epoch 585 - Train Loss: 0.0626 - Val Loss: 0.0635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 586, Train Loss: 0.0350\n",
      "Epoch 586 - Train Loss: 0.0350 - Val Loss: 0.0359\n",
      "Epoch: 587, Train Loss: 0.0475\n",
      "Epoch 587 - Train Loss: 0.0475 - Val Loss: 0.0218\n",
      "Epoch: 588, Train Loss: 0.0493\n",
      "Epoch 588 - Train Loss: 0.0493 - Val Loss: 0.0905\n",
      "Epoch: 589, Train Loss: 0.0521\n",
      "Epoch 589 - Train Loss: 0.0521 - Val Loss: 0.0574\n",
      "Epoch: 590, Train Loss: 0.0456\n",
      "Epoch 590 - Train Loss: 0.0456 - Val Loss: 0.0261\n",
      "Epoch: 591, Train Loss: 0.0480\n",
      "Epoch 591 - Train Loss: 0.0480 - Val Loss: 0.0448\n",
      "Epoch: 592, Train Loss: 0.0480\n",
      "Epoch 592 - Train Loss: 0.0480 - Val Loss: 0.0471\n",
      "Epoch: 593, Train Loss: 0.0469\n",
      "Epoch 593 - Train Loss: 0.0469 - Val Loss: 0.0778\n",
      "Epoch: 594, Train Loss: 0.0529\n",
      "Epoch 594 - Train Loss: 0.0529 - Val Loss: 0.0286\n",
      "Epoch: 595, Train Loss: 0.0550\n",
      "Epoch 595 - Train Loss: 0.0550 - Val Loss: 0.0175\n",
      "Epoch: 596, Train Loss: 0.0483\n",
      "Epoch 596 - Train Loss: 0.0483 - Val Loss: 0.0456\n",
      "Epoch: 597, Train Loss: 0.0473\n",
      "Epoch 597 - Train Loss: 0.0473 - Val Loss: 0.0403\n",
      "Epoch: 598, Train Loss: 0.0427\n",
      "Epoch 598 - Train Loss: 0.0427 - Val Loss: 0.0314\n",
      "Epoch: 599, Train Loss: 0.0424\n",
      "Epoch 599 - Train Loss: 0.0424 - Val Loss: 0.0250\n",
      "Epoch: 600, Train Loss: 0.0497\n",
      "Epoch 600 - Train Loss: 0.0497 - Val Loss: 0.0304\n",
      "Epoch: 601, Train Loss: 0.0577\n",
      "Epoch 601 - Train Loss: 0.0577 - Val Loss: 0.0731\n",
      "Epoch: 602, Train Loss: 0.0380\n",
      "Epoch 602 - Train Loss: 0.0380 - Val Loss: 0.0186\n",
      "Epoch: 603, Train Loss: 0.0500\n",
      "Epoch 603 - Train Loss: 0.0500 - Val Loss: 0.0330\n",
      "Epoch: 604, Train Loss: 0.0422\n",
      "Epoch 604 - Train Loss: 0.0422 - Val Loss: 0.0080\n",
      "Epoch: 605, Train Loss: 0.0646\n",
      "Epoch 605 - Train Loss: 0.0646 - Val Loss: 0.0458\n",
      "Epoch: 606, Train Loss: 0.0363\n",
      "Epoch 606 - Train Loss: 0.0363 - Val Loss: 0.0330\n",
      "Epoch: 607, Train Loss: 0.0385\n",
      "Epoch 607 - Train Loss: 0.0385 - Val Loss: 0.0288\n",
      "Epoch: 608, Train Loss: 0.0337\n",
      "Epoch 608 - Train Loss: 0.0337 - Val Loss: 0.0508\n",
      "Epoch: 609, Train Loss: 0.0370\n",
      "Epoch 609 - Train Loss: 0.0370 - Val Loss: 0.0155\n",
      "Epoch: 610, Train Loss: 0.0459\n",
      "Epoch 610 - Train Loss: 0.0459 - Val Loss: 0.0704\n",
      "Epoch: 611, Train Loss: 0.0504\n",
      "Epoch 611 - Train Loss: 0.0504 - Val Loss: 0.0595\n",
      "Epoch: 612, Train Loss: 0.0428\n",
      "Epoch 612 - Train Loss: 0.0428 - Val Loss: 0.0489\n",
      "Epoch: 613, Train Loss: 0.0486\n",
      "Epoch 613 - Train Loss: 0.0486 - Val Loss: 0.0348\n",
      "Epoch: 614, Train Loss: 0.0324\n",
      "Epoch 614 - Train Loss: 0.0324 - Val Loss: 0.0310\n",
      "Epoch: 615, Train Loss: 0.0439\n",
      "Epoch 615 - Train Loss: 0.0439 - Val Loss: 0.0277\n",
      "Epoch: 616, Train Loss: 0.0502\n",
      "Epoch 616 - Train Loss: 0.0502 - Val Loss: 0.0446\n",
      "Epoch: 617, Train Loss: 0.0556\n",
      "Epoch 617 - Train Loss: 0.0556 - Val Loss: 0.0252\n",
      "Epoch: 618, Train Loss: 0.0378\n",
      "Epoch 618 - Train Loss: 0.0378 - Val Loss: 0.0326\n",
      "Epoch: 619, Train Loss: 0.0485\n",
      "Epoch 619 - Train Loss: 0.0485 - Val Loss: 0.0465\n",
      "Epoch: 620, Train Loss: 0.0412\n",
      "Epoch 620 - Train Loss: 0.0412 - Val Loss: 0.0804\n",
      "Epoch: 621, Train Loss: 0.0502\n",
      "Epoch 621 - Train Loss: 0.0502 - Val Loss: 0.0406\n",
      "Epoch: 622, Train Loss: 0.0393\n",
      "Epoch 622 - Train Loss: 0.0393 - Val Loss: 0.0317\n",
      "Epoch: 623, Train Loss: 0.0545\n",
      "Epoch 623 - Train Loss: 0.0545 - Val Loss: 0.0443\n",
      "Epoch: 624, Train Loss: 0.0367\n",
      "Epoch 624 - Train Loss: 0.0367 - Val Loss: 0.0410\n",
      "Epoch: 625, Train Loss: 0.0464\n",
      "Epoch 625 - Train Loss: 0.0464 - Val Loss: 0.0288\n",
      "Epoch: 626, Train Loss: 0.0426\n",
      "Epoch 626 - Train Loss: 0.0426 - Val Loss: 0.0453\n",
      "Epoch: 627, Train Loss: 0.0470\n",
      "Epoch 627 - Train Loss: 0.0470 - Val Loss: 0.0264\n",
      "Epoch: 628, Train Loss: 0.0469\n",
      "Epoch 628 - Train Loss: 0.0469 - Val Loss: 0.0504\n",
      "Epoch: 629, Train Loss: 0.0530\n",
      "Epoch 629 - Train Loss: 0.0530 - Val Loss: 0.0580\n",
      "Epoch: 630, Train Loss: 0.0468\n",
      "Epoch 630 - Train Loss: 0.0468 - Val Loss: 0.0347\n",
      "Epoch: 631, Train Loss: 0.0403\n",
      "Epoch 631 - Train Loss: 0.0403 - Val Loss: 0.0191\n",
      "Epoch: 632, Train Loss: 0.0381\n",
      "Epoch 632 - Train Loss: 0.0381 - Val Loss: 0.0139\n",
      "Epoch: 633, Train Loss: 0.0467\n",
      "Epoch 633 - Train Loss: 0.0467 - Val Loss: 0.0316\n",
      "Epoch: 634, Train Loss: 0.0504\n",
      "Epoch 634 - Train Loss: 0.0504 - Val Loss: 0.0458\n",
      "Epoch: 635, Train Loss: 0.0431\n",
      "Epoch 635 - Train Loss: 0.0431 - Val Loss: 0.0349\n",
      "Epoch: 636, Train Loss: 0.0396\n",
      "Epoch 636 - Train Loss: 0.0396 - Val Loss: 0.0096\n",
      "Epoch: 637, Train Loss: 0.0390\n",
      "Epoch 637 - Train Loss: 0.0390 - Val Loss: 0.0646\n",
      "Epoch: 638, Train Loss: 0.0398\n",
      "Epoch 638 - Train Loss: 0.0398 - Val Loss: 0.0546\n",
      "Epoch: 639, Train Loss: 0.0395\n",
      "Epoch 639 - Train Loss: 0.0395 - Val Loss: 0.0430\n",
      "Epoch: 640, Train Loss: 0.0417\n",
      "Epoch 640 - Train Loss: 0.0417 - Val Loss: 0.0219\n",
      "Epoch: 641, Train Loss: 0.0454\n",
      "Epoch 641 - Train Loss: 0.0454 - Val Loss: 0.0443\n",
      "Epoch: 642, Train Loss: 0.0438\n",
      "Epoch 642 - Train Loss: 0.0438 - Val Loss: 0.0286\n",
      "Epoch: 643, Train Loss: 0.0453\n",
      "Epoch 643 - Train Loss: 0.0453 - Val Loss: 0.0240\n",
      "Epoch: 644, Train Loss: 0.0483\n",
      "Epoch 644 - Train Loss: 0.0483 - Val Loss: 0.0576\n",
      "Epoch: 645, Train Loss: 0.0370\n",
      "Epoch 645 - Train Loss: 0.0370 - Val Loss: 0.0294\n",
      "Epoch: 646, Train Loss: 0.0412\n",
      "Epoch 646 - Train Loss: 0.0412 - Val Loss: 0.0294\n",
      "Epoch: 647, Train Loss: 0.0391\n",
      "Epoch 647 - Train Loss: 0.0391 - Val Loss: 0.0283\n",
      "Epoch: 648, Train Loss: 0.0358\n",
      "Epoch 648 - Train Loss: 0.0358 - Val Loss: 0.0371\n",
      "Epoch: 649, Train Loss: 0.0391\n",
      "Epoch 649 - Train Loss: 0.0391 - Val Loss: 0.0347\n",
      "Epoch: 650, Train Loss: 0.0335\n",
      "Epoch 650 - Train Loss: 0.0335 - Val Loss: 0.0253\n",
      "Epoch: 651, Train Loss: 0.0505\n",
      "Epoch 651 - Train Loss: 0.0505 - Val Loss: 0.0335\n",
      "Epoch: 652, Train Loss: 0.0364\n",
      "Epoch 652 - Train Loss: 0.0364 - Val Loss: 0.0239\n",
      "Epoch: 653, Train Loss: 0.0396\n",
      "Epoch 653 - Train Loss: 0.0396 - Val Loss: 0.0353\n",
      "Epoch: 654, Train Loss: 0.0446\n",
      "Epoch 654 - Train Loss: 0.0446 - Val Loss: 0.0280\n",
      "Epoch: 655, Train Loss: 0.0502\n",
      "Epoch 655 - Train Loss: 0.0502 - Val Loss: 0.0306\n",
      "Epoch: 656, Train Loss: 0.0667\n",
      "Epoch 656 - Train Loss: 0.0667 - Val Loss: 0.0495\n",
      "Epoch: 657, Train Loss: 0.0564\n",
      "Epoch 657 - Train Loss: 0.0564 - Val Loss: 0.0335\n",
      "Epoch: 658, Train Loss: 0.0329\n",
      "Epoch 658 - Train Loss: 0.0329 - Val Loss: 0.0193\n",
      "Epoch: 659, Train Loss: 0.0322\n",
      "Epoch 659 - Train Loss: 0.0322 - Val Loss: 0.0208\n",
      "Epoch: 660, Train Loss: 0.0389\n",
      "Epoch 660 - Train Loss: 0.0389 - Val Loss: 0.0336\n",
      "Epoch: 661, Train Loss: 0.0388\n",
      "Epoch 661 - Train Loss: 0.0388 - Val Loss: 0.0257\n",
      "Epoch: 662, Train Loss: 0.0433\n",
      "Epoch 662 - Train Loss: 0.0433 - Val Loss: 0.0725\n",
      "Epoch: 663, Train Loss: 0.0373\n",
      "Epoch 663 - Train Loss: 0.0373 - Val Loss: 0.0333\n",
      "Epoch: 664, Train Loss: 0.0411\n",
      "Epoch 664 - Train Loss: 0.0411 - Val Loss: 0.0252\n",
      "Epoch: 665, Train Loss: 0.0371\n",
      "Epoch 665 - Train Loss: 0.0371 - Val Loss: 0.0422\n",
      "Epoch: 666, Train Loss: 0.0381\n",
      "Epoch 666 - Train Loss: 0.0381 - Val Loss: 0.0231\n",
      "Epoch: 667, Train Loss: 0.0452\n",
      "Epoch 667 - Train Loss: 0.0452 - Val Loss: 0.0238\n",
      "Epoch: 668, Train Loss: 0.0462\n",
      "Epoch 668 - Train Loss: 0.0462 - Val Loss: 0.0399\n",
      "Epoch: 669, Train Loss: 0.0415\n",
      "Epoch 669 - Train Loss: 0.0415 - Val Loss: 0.0527\n",
      "Epoch: 670, Train Loss: 0.0463\n",
      "Epoch 670 - Train Loss: 0.0463 - Val Loss: 0.0587\n",
      "Epoch: 671, Train Loss: 0.0498\n",
      "Epoch 671 - Train Loss: 0.0498 - Val Loss: 0.0720\n",
      "Epoch: 672, Train Loss: 0.0385\n",
      "Epoch 672 - Train Loss: 0.0385 - Val Loss: 0.0298\n",
      "Epoch: 673, Train Loss: 0.0345\n",
      "Epoch 673 - Train Loss: 0.0345 - Val Loss: 0.0383\n",
      "Epoch: 674, Train Loss: 0.0464\n",
      "Epoch 674 - Train Loss: 0.0464 - Val Loss: 0.0498\n",
      "Epoch: 675, Train Loss: 0.0355\n",
      "Epoch 675 - Train Loss: 0.0355 - Val Loss: 0.0384\n",
      "Epoch: 676, Train Loss: 0.0407\n",
      "Epoch 676 - Train Loss: 0.0407 - Val Loss: 0.0301\n",
      "Epoch: 677, Train Loss: 0.0363\n",
      "Epoch 677 - Train Loss: 0.0363 - Val Loss: 0.0211\n",
      "Epoch: 678, Train Loss: 0.0347\n",
      "Epoch 678 - Train Loss: 0.0347 - Val Loss: 0.0353\n",
      "Epoch: 679, Train Loss: 0.0370\n",
      "Epoch 679 - Train Loss: 0.0370 - Val Loss: 0.0169\n",
      "Epoch: 680, Train Loss: 0.0378\n",
      "Epoch 680 - Train Loss: 0.0378 - Val Loss: 0.0521\n",
      "Epoch: 681, Train Loss: 0.0300\n",
      "Epoch 681 - Train Loss: 0.0300 - Val Loss: 0.0187\n",
      "Epoch: 682, Train Loss: 0.0427\n",
      "Epoch 682 - Train Loss: 0.0427 - Val Loss: 0.0215\n",
      "Epoch: 683, Train Loss: 0.0359\n",
      "Epoch 683 - Train Loss: 0.0359 - Val Loss: 0.0046\n",
      "Epoch: 684, Train Loss: 0.0383\n",
      "Epoch 684 - Train Loss: 0.0383 - Val Loss: 0.0206\n",
      "Epoch: 685, Train Loss: 0.0483\n",
      "Epoch 685 - Train Loss: 0.0483 - Val Loss: 0.0799\n",
      "Epoch: 686, Train Loss: 0.0241\n",
      "Epoch 686 - Train Loss: 0.0241 - Val Loss: 0.0402\n",
      "Epoch: 687, Train Loss: 0.0309\n",
      "Epoch 687 - Train Loss: 0.0309 - Val Loss: 0.0286\n",
      "Epoch: 688, Train Loss: 0.0431\n",
      "Epoch 688 - Train Loss: 0.0431 - Val Loss: 0.0396\n",
      "Epoch: 689, Train Loss: 0.0490\n",
      "Epoch 689 - Train Loss: 0.0490 - Val Loss: 0.0603\n",
      "Epoch: 690, Train Loss: 0.0393\n",
      "Epoch 690 - Train Loss: 0.0393 - Val Loss: 0.0208\n",
      "Epoch: 691, Train Loss: 0.0340\n",
      "Epoch 691 - Train Loss: 0.0340 - Val Loss: 0.0467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 692, Train Loss: 0.0407\n",
      "Epoch 692 - Train Loss: 0.0407 - Val Loss: 0.0175\n",
      "Epoch: 693, Train Loss: 0.0429\n",
      "Epoch 693 - Train Loss: 0.0429 - Val Loss: 0.0361\n",
      "Epoch: 694, Train Loss: 0.0390\n",
      "Epoch 694 - Train Loss: 0.0390 - Val Loss: 0.0425\n",
      "Epoch: 695, Train Loss: 0.0333\n",
      "Epoch 695 - Train Loss: 0.0333 - Val Loss: 0.0088\n",
      "Epoch: 696, Train Loss: 0.0406\n",
      "Epoch 696 - Train Loss: 0.0406 - Val Loss: 0.0374\n",
      "Epoch: 697, Train Loss: 0.0374\n",
      "Epoch 697 - Train Loss: 0.0374 - Val Loss: 0.0260\n",
      "Epoch: 698, Train Loss: 0.0404\n",
      "Epoch 698 - Train Loss: 0.0404 - Val Loss: 0.0464\n",
      "Epoch: 699, Train Loss: 0.0363\n",
      "Epoch 699 - Train Loss: 0.0363 - Val Loss: 0.0179\n",
      "Epoch: 700, Train Loss: 0.0314\n",
      "Epoch 700 - Train Loss: 0.0314 - Val Loss: 0.0107\n",
      "Epoch: 701, Train Loss: 0.0400\n",
      "Epoch 701 - Train Loss: 0.0400 - Val Loss: 0.0106\n",
      "Epoch: 702, Train Loss: 0.0419\n",
      "Epoch 702 - Train Loss: 0.0419 - Val Loss: 0.0373\n",
      "Epoch: 703, Train Loss: 0.0485\n",
      "Epoch 703 - Train Loss: 0.0485 - Val Loss: 0.0373\n",
      "Epoch: 704, Train Loss: 0.0403\n",
      "Epoch 704 - Train Loss: 0.0403 - Val Loss: 0.0342\n",
      "Epoch: 705, Train Loss: 0.0427\n",
      "Epoch 705 - Train Loss: 0.0427 - Val Loss: 0.0201\n",
      "Epoch: 706, Train Loss: 0.0367\n",
      "Epoch 706 - Train Loss: 0.0367 - Val Loss: 0.0302\n",
      "Epoch: 707, Train Loss: 0.0443\n",
      "Epoch 707 - Train Loss: 0.0443 - Val Loss: 0.0379\n",
      "Epoch: 708, Train Loss: 0.0354\n",
      "Epoch 708 - Train Loss: 0.0354 - Val Loss: 0.0376\n",
      "Epoch: 709, Train Loss: 0.0405\n",
      "Epoch 709 - Train Loss: 0.0405 - Val Loss: 0.0239\n",
      "Epoch: 710, Train Loss: 0.0347\n",
      "Epoch 710 - Train Loss: 0.0347 - Val Loss: 0.0252\n",
      "Epoch: 711, Train Loss: 0.0316\n",
      "Epoch 711 - Train Loss: 0.0316 - Val Loss: 0.0375\n",
      "Epoch: 712, Train Loss: 0.0378\n",
      "Epoch 712 - Train Loss: 0.0378 - Val Loss: 0.0188\n",
      "Epoch: 713, Train Loss: 0.0424\n",
      "Epoch 713 - Train Loss: 0.0424 - Val Loss: 0.0237\n",
      "Epoch: 714, Train Loss: 0.0320\n",
      "Epoch 714 - Train Loss: 0.0320 - Val Loss: 0.0051\n",
      "Epoch: 715, Train Loss: 0.0301\n",
      "Epoch 715 - Train Loss: 0.0301 - Val Loss: 0.0133\n",
      "Epoch: 716, Train Loss: 0.0347\n",
      "Epoch 716 - Train Loss: 0.0347 - Val Loss: 0.0128\n",
      "Epoch: 717, Train Loss: 0.0383\n",
      "Epoch 717 - Train Loss: 0.0383 - Val Loss: 0.0608\n",
      "Epoch: 718, Train Loss: 0.0325\n",
      "Epoch 718 - Train Loss: 0.0325 - Val Loss: 0.0148\n",
      "Epoch: 719, Train Loss: 0.0352\n",
      "Epoch 719 - Train Loss: 0.0352 - Val Loss: 0.0333\n",
      "Epoch: 720, Train Loss: 0.0384\n",
      "Epoch 720 - Train Loss: 0.0384 - Val Loss: 0.0462\n",
      "Epoch: 721, Train Loss: 0.0393\n",
      "Epoch 721 - Train Loss: 0.0393 - Val Loss: 0.0295\n",
      "Epoch: 722, Train Loss: 0.0437\n",
      "Epoch 722 - Train Loss: 0.0437 - Val Loss: 0.0185\n",
      "Epoch: 723, Train Loss: 0.0471\n",
      "Epoch 723 - Train Loss: 0.0471 - Val Loss: 0.0256\n",
      "Epoch: 724, Train Loss: 0.0396\n",
      "Epoch 724 - Train Loss: 0.0396 - Val Loss: 0.0291\n",
      "Epoch: 725, Train Loss: 0.0340\n",
      "Epoch 725 - Train Loss: 0.0340 - Val Loss: 0.0319\n",
      "Epoch: 726, Train Loss: 0.0343\n",
      "Epoch 726 - Train Loss: 0.0343 - Val Loss: 0.0409\n",
      "Epoch: 727, Train Loss: 0.0286\n",
      "Epoch 727 - Train Loss: 0.0286 - Val Loss: 0.0158\n",
      "Epoch: 728, Train Loss: 0.0324\n",
      "Epoch 728 - Train Loss: 0.0324 - Val Loss: 0.0346\n",
      "Epoch: 729, Train Loss: 0.0409\n",
      "Epoch 729 - Train Loss: 0.0409 - Val Loss: 0.0498\n",
      "Epoch: 730, Train Loss: 0.0388\n",
      "Epoch 730 - Train Loss: 0.0388 - Val Loss: 0.0615\n",
      "Epoch: 731, Train Loss: 0.0340\n",
      "Epoch 731 - Train Loss: 0.0340 - Val Loss: 0.0328\n",
      "Epoch: 732, Train Loss: 0.0372\n",
      "Epoch 732 - Train Loss: 0.0372 - Val Loss: 0.0305\n",
      "Epoch: 733, Train Loss: 0.0299\n",
      "Epoch 733 - Train Loss: 0.0299 - Val Loss: 0.0207\n",
      "Epoch: 734, Train Loss: 0.0390\n",
      "Epoch 734 - Train Loss: 0.0390 - Val Loss: 0.0559\n",
      "Epoch: 735, Train Loss: 0.0436\n",
      "Epoch 735 - Train Loss: 0.0436 - Val Loss: 0.0457\n",
      "Epoch: 736, Train Loss: 0.0370\n",
      "Epoch 736 - Train Loss: 0.0370 - Val Loss: 0.0282\n",
      "Epoch: 737, Train Loss: 0.0343\n",
      "Epoch 737 - Train Loss: 0.0343 - Val Loss: 0.0287\n",
      "Epoch: 738, Train Loss: 0.0314\n",
      "Epoch 738 - Train Loss: 0.0314 - Val Loss: 0.0149\n",
      "Epoch: 739, Train Loss: 0.0343\n",
      "Epoch 739 - Train Loss: 0.0343 - Val Loss: 0.0230\n",
      "Epoch: 740, Train Loss: 0.0330\n",
      "Epoch 740 - Train Loss: 0.0330 - Val Loss: 0.0258\n",
      "Epoch: 741, Train Loss: 0.0304\n",
      "Epoch 741 - Train Loss: 0.0304 - Val Loss: 0.0337\n",
      "Epoch: 742, Train Loss: 0.0278\n",
      "Epoch 742 - Train Loss: 0.0278 - Val Loss: 0.0253\n",
      "Epoch: 743, Train Loss: 0.0298\n",
      "Epoch 743 - Train Loss: 0.0298 - Val Loss: 0.0232\n",
      "Epoch: 744, Train Loss: 0.0407\n",
      "Epoch 744 - Train Loss: 0.0407 - Val Loss: 0.0351\n",
      "Epoch: 745, Train Loss: 0.0447\n",
      "Epoch 745 - Train Loss: 0.0447 - Val Loss: 0.0341\n",
      "Epoch: 746, Train Loss: 0.0323\n",
      "Epoch 746 - Train Loss: 0.0323 - Val Loss: 0.0254\n",
      "Epoch: 747, Train Loss: 0.0360\n",
      "Epoch 747 - Train Loss: 0.0360 - Val Loss: 0.0637\n",
      "Epoch: 748, Train Loss: 0.0290\n",
      "Epoch 748 - Train Loss: 0.0290 - Val Loss: 0.0474\n",
      "Epoch: 749, Train Loss: 0.0315\n",
      "Epoch 749 - Train Loss: 0.0315 - Val Loss: 0.0376\n",
      "Epoch: 750, Train Loss: 0.0405\n",
      "Epoch 750 - Train Loss: 0.0405 - Val Loss: 0.0613\n",
      "Epoch: 751, Train Loss: 0.0362\n",
      "Epoch 751 - Train Loss: 0.0362 - Val Loss: 0.0216\n",
      "Epoch: 752, Train Loss: 0.0285\n",
      "Epoch 752 - Train Loss: 0.0285 - Val Loss: 0.0342\n",
      "Epoch: 753, Train Loss: 0.0387\n",
      "Epoch 753 - Train Loss: 0.0387 - Val Loss: 0.0256\n",
      "Epoch: 754, Train Loss: 0.0329\n",
      "Epoch 754 - Train Loss: 0.0329 - Val Loss: 0.0072\n",
      "Epoch: 755, Train Loss: 0.0312\n",
      "Epoch 755 - Train Loss: 0.0312 - Val Loss: 0.0366\n",
      "Epoch: 756, Train Loss: 0.0351\n",
      "Epoch 756 - Train Loss: 0.0351 - Val Loss: 0.0474\n",
      "Epoch: 757, Train Loss: 0.0326\n",
      "Epoch 757 - Train Loss: 0.0326 - Val Loss: 0.0160\n",
      "Epoch: 758, Train Loss: 0.0367\n",
      "Epoch 758 - Train Loss: 0.0367 - Val Loss: 0.0343\n",
      "Epoch: 759, Train Loss: 0.0357\n",
      "Epoch 759 - Train Loss: 0.0357 - Val Loss: 0.0392\n",
      "Epoch: 760, Train Loss: 0.0239\n",
      "Epoch 760 - Train Loss: 0.0239 - Val Loss: 0.0164\n",
      "Epoch: 761, Train Loss: 0.0368\n",
      "Epoch 761 - Train Loss: 0.0368 - Val Loss: 0.0090\n",
      "Epoch: 762, Train Loss: 0.0338\n",
      "Epoch 762 - Train Loss: 0.0338 - Val Loss: 0.0400\n",
      "Epoch: 763, Train Loss: 0.0289\n",
      "Epoch 763 - Train Loss: 0.0289 - Val Loss: 0.0319\n",
      "Epoch: 764, Train Loss: 0.0270\n",
      "Epoch 764 - Train Loss: 0.0270 - Val Loss: 0.0253\n",
      "Epoch: 765, Train Loss: 0.0333\n",
      "Epoch 765 - Train Loss: 0.0333 - Val Loss: 0.0217\n",
      "Epoch: 766, Train Loss: 0.0309\n",
      "Epoch 766 - Train Loss: 0.0309 - Val Loss: 0.0472\n",
      "Epoch: 767, Train Loss: 0.0402\n",
      "Epoch 767 - Train Loss: 0.0402 - Val Loss: 0.0165\n",
      "Epoch: 768, Train Loss: 0.0444\n",
      "Epoch 768 - Train Loss: 0.0444 - Val Loss: 0.0343\n",
      "Epoch: 769, Train Loss: 0.0362\n",
      "Epoch 769 - Train Loss: 0.0362 - Val Loss: 0.0468\n",
      "Epoch: 770, Train Loss: 0.0424\n",
      "Epoch 770 - Train Loss: 0.0424 - Val Loss: 0.0471\n",
      "Epoch: 771, Train Loss: 0.0355\n",
      "Epoch 771 - Train Loss: 0.0355 - Val Loss: 0.0226\n",
      "Epoch: 772, Train Loss: 0.0329\n",
      "Epoch 772 - Train Loss: 0.0329 - Val Loss: 0.0305\n",
      "Epoch: 773, Train Loss: 0.0371\n",
      "Epoch 773 - Train Loss: 0.0371 - Val Loss: 0.0225\n",
      "Epoch: 774, Train Loss: 0.0387\n",
      "Epoch 774 - Train Loss: 0.0387 - Val Loss: 0.0176\n",
      "Epoch: 775, Train Loss: 0.0359\n",
      "Epoch 775 - Train Loss: 0.0359 - Val Loss: 0.0329\n",
      "Epoch: 776, Train Loss: 0.0243\n",
      "Epoch 776 - Train Loss: 0.0243 - Val Loss: 0.0267\n",
      "Epoch: 777, Train Loss: 0.0387\n",
      "Epoch 777 - Train Loss: 0.0387 - Val Loss: 0.0333\n",
      "Epoch: 778, Train Loss: 0.0348\n",
      "Epoch 778 - Train Loss: 0.0348 - Val Loss: 0.0443\n",
      "Epoch: 779, Train Loss: 0.0372\n",
      "Epoch 779 - Train Loss: 0.0372 - Val Loss: 0.0454\n",
      "Epoch: 780, Train Loss: 0.0361\n",
      "Epoch 780 - Train Loss: 0.0361 - Val Loss: 0.0264\n",
      "Epoch: 781, Train Loss: 0.0300\n",
      "Epoch 781 - Train Loss: 0.0300 - Val Loss: 0.0246\n",
      "Epoch: 782, Train Loss: 0.0276\n",
      "Epoch 782 - Train Loss: 0.0276 - Val Loss: 0.0233\n",
      "Epoch: 783, Train Loss: 0.0295\n",
      "Epoch 783 - Train Loss: 0.0295 - Val Loss: 0.0126\n",
      "Epoch: 784, Train Loss: 0.0248\n",
      "Epoch 784 - Train Loss: 0.0248 - Val Loss: 0.0178\n",
      "Epoch: 785, Train Loss: 0.0289\n",
      "Epoch 785 - Train Loss: 0.0289 - Val Loss: 0.0135\n",
      "Epoch: 786, Train Loss: 0.0334\n",
      "Epoch 786 - Train Loss: 0.0334 - Val Loss: 0.0341\n",
      "Epoch: 787, Train Loss: 0.0327\n",
      "Epoch 787 - Train Loss: 0.0327 - Val Loss: 0.0302\n",
      "Epoch: 788, Train Loss: 0.0277\n",
      "Epoch 788 - Train Loss: 0.0277 - Val Loss: 0.0208\n",
      "Epoch: 789, Train Loss: 0.0326\n",
      "Epoch 789 - Train Loss: 0.0326 - Val Loss: 0.0205\n",
      "Epoch: 790, Train Loss: 0.0292\n",
      "Epoch 790 - Train Loss: 0.0292 - Val Loss: 0.0116\n",
      "Epoch: 791, Train Loss: 0.0351\n",
      "Epoch 791 - Train Loss: 0.0351 - Val Loss: 0.0393\n",
      "Epoch: 792, Train Loss: 0.0423\n",
      "Epoch 792 - Train Loss: 0.0423 - Val Loss: 0.0219\n",
      "Epoch: 793, Train Loss: 0.0341\n",
      "Epoch 793 - Train Loss: 0.0341 - Val Loss: 0.0613\n",
      "Epoch: 794, Train Loss: 0.0385\n",
      "Epoch 794 - Train Loss: 0.0385 - Val Loss: 0.0236\n",
      "Epoch: 795, Train Loss: 0.0300\n",
      "Epoch 795 - Train Loss: 0.0300 - Val Loss: 0.0097\n",
      "Epoch: 796, Train Loss: 0.0340\n",
      "Epoch 796 - Train Loss: 0.0340 - Val Loss: 0.0311\n",
      "Epoch: 797, Train Loss: 0.0332\n",
      "Epoch 797 - Train Loss: 0.0332 - Val Loss: 0.0338\n",
      "Epoch: 798, Train Loss: 0.0310\n",
      "Epoch 798 - Train Loss: 0.0310 - Val Loss: 0.0225\n",
      "Epoch: 799, Train Loss: 0.0358\n",
      "Epoch 799 - Train Loss: 0.0358 - Val Loss: 0.0437\n",
      "Epoch: 800, Train Loss: 0.0385\n",
      "Epoch 800 - Train Loss: 0.0385 - Val Loss: 0.0466\n",
      "Epoch: 801, Train Loss: 0.0301\n",
      "Epoch 801 - Train Loss: 0.0301 - Val Loss: 0.0340\n",
      "Epoch: 802, Train Loss: 0.0294\n",
      "Epoch 802 - Train Loss: 0.0294 - Val Loss: 0.0324\n",
      "Epoch: 803, Train Loss: 0.0351\n",
      "Epoch 803 - Train Loss: 0.0351 - Val Loss: 0.0211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 804, Train Loss: 0.0305\n",
      "Epoch 804 - Train Loss: 0.0305 - Val Loss: 0.0421\n",
      "Epoch: 805, Train Loss: 0.0328\n",
      "Epoch 805 - Train Loss: 0.0328 - Val Loss: 0.0209\n",
      "Epoch: 806, Train Loss: 0.0248\n",
      "Epoch 806 - Train Loss: 0.0248 - Val Loss: 0.0241\n",
      "Epoch: 807, Train Loss: 0.0253\n",
      "Epoch 807 - Train Loss: 0.0253 - Val Loss: 0.0223\n",
      "Epoch: 808, Train Loss: 0.0294\n",
      "Epoch 808 - Train Loss: 0.0294 - Val Loss: 0.0160\n",
      "Epoch: 809, Train Loss: 0.0359\n",
      "Epoch 809 - Train Loss: 0.0359 - Val Loss: 0.0291\n",
      "Epoch: 810, Train Loss: 0.0244\n",
      "Epoch 810 - Train Loss: 0.0244 - Val Loss: 0.0122\n",
      "Epoch: 811, Train Loss: 0.0366\n",
      "Epoch 811 - Train Loss: 0.0366 - Val Loss: 0.0394\n",
      "Epoch: 812, Train Loss: 0.0380\n",
      "Epoch 812 - Train Loss: 0.0380 - Val Loss: 0.0265\n",
      "Epoch: 813, Train Loss: 0.0315\n",
      "Epoch 813 - Train Loss: 0.0315 - Val Loss: 0.0465\n",
      "Epoch: 814, Train Loss: 0.0301\n",
      "Epoch 814 - Train Loss: 0.0301 - Val Loss: 0.0554\n",
      "Epoch: 815, Train Loss: 0.0279\n",
      "Epoch 815 - Train Loss: 0.0279 - Val Loss: 0.0352\n",
      "Epoch: 816, Train Loss: 0.0281\n",
      "Epoch 816 - Train Loss: 0.0281 - Val Loss: 0.0167\n",
      "Epoch: 817, Train Loss: 0.0338\n",
      "Epoch 817 - Train Loss: 0.0338 - Val Loss: 0.0289\n",
      "Epoch: 818, Train Loss: 0.0287\n",
      "Epoch 818 - Train Loss: 0.0287 - Val Loss: 0.0200\n",
      "Epoch: 819, Train Loss: 0.0255\n",
      "Epoch 819 - Train Loss: 0.0255 - Val Loss: 0.0241\n",
      "Epoch: 820, Train Loss: 0.0265\n",
      "Epoch 820 - Train Loss: 0.0265 - Val Loss: 0.0160\n",
      "Epoch: 821, Train Loss: 0.0259\n",
      "Epoch 821 - Train Loss: 0.0259 - Val Loss: 0.0276\n",
      "Epoch: 822, Train Loss: 0.0326\n",
      "Epoch 822 - Train Loss: 0.0326 - Val Loss: 0.0306\n",
      "Epoch: 823, Train Loss: 0.0357\n",
      "Epoch 823 - Train Loss: 0.0357 - Val Loss: 0.0466\n",
      "Epoch: 824, Train Loss: 0.0295\n",
      "Epoch 824 - Train Loss: 0.0295 - Val Loss: 0.0203\n",
      "Epoch: 825, Train Loss: 0.0271\n",
      "Epoch 825 - Train Loss: 0.0271 - Val Loss: 0.0310\n",
      "Epoch: 826, Train Loss: 0.0298\n",
      "Epoch 826 - Train Loss: 0.0298 - Val Loss: 0.0172\n",
      "Epoch: 827, Train Loss: 0.0281\n",
      "Epoch 827 - Train Loss: 0.0281 - Val Loss: 0.0172\n",
      "Epoch: 828, Train Loss: 0.0244\n",
      "Epoch 828 - Train Loss: 0.0244 - Val Loss: 0.0202\n",
      "Epoch: 829, Train Loss: 0.0395\n",
      "Epoch 829 - Train Loss: 0.0395 - Val Loss: 0.0605\n",
      "Epoch: 830, Train Loss: 0.0236\n",
      "Epoch 830 - Train Loss: 0.0236 - Val Loss: 0.0255\n",
      "Epoch: 831, Train Loss: 0.0284\n",
      "Epoch 831 - Train Loss: 0.0284 - Val Loss: 0.0243\n",
      "Epoch: 832, Train Loss: 0.0288\n",
      "Epoch 832 - Train Loss: 0.0288 - Val Loss: 0.0155\n",
      "Epoch: 833, Train Loss: 0.0285\n",
      "Epoch 833 - Train Loss: 0.0285 - Val Loss: 0.0277\n",
      "Epoch: 834, Train Loss: 0.0360\n",
      "Epoch 834 - Train Loss: 0.0360 - Val Loss: 0.0428\n",
      "Epoch: 835, Train Loss: 0.0224\n",
      "Epoch 835 - Train Loss: 0.0224 - Val Loss: 0.0315\n",
      "Epoch: 836, Train Loss: 0.0299\n",
      "Epoch 836 - Train Loss: 0.0299 - Val Loss: 0.0170\n",
      "Epoch: 837, Train Loss: 0.0369\n",
      "Epoch 837 - Train Loss: 0.0369 - Val Loss: 0.0150\n",
      "Epoch: 838, Train Loss: 0.0360\n",
      "Epoch 838 - Train Loss: 0.0360 - Val Loss: 0.0262\n",
      "Epoch: 839, Train Loss: 0.0264\n",
      "Epoch 839 - Train Loss: 0.0264 - Val Loss: 0.0211\n",
      "Epoch: 840, Train Loss: 0.0276\n",
      "Epoch 840 - Train Loss: 0.0276 - Val Loss: 0.0059\n",
      "Epoch: 841, Train Loss: 0.0351\n",
      "Epoch 841 - Train Loss: 0.0351 - Val Loss: 0.0314\n",
      "Epoch: 842, Train Loss: 0.0341\n",
      "Epoch 842 - Train Loss: 0.0341 - Val Loss: 0.0131\n",
      "Epoch: 843, Train Loss: 0.0239\n",
      "Epoch 843 - Train Loss: 0.0239 - Val Loss: 0.0183\n",
      "Epoch: 844, Train Loss: 0.0194\n",
      "Epoch 844 - Train Loss: 0.0194 - Val Loss: 0.0163\n",
      "Epoch: 845, Train Loss: 0.0432\n",
      "Epoch 845 - Train Loss: 0.0432 - Val Loss: 0.0358\n",
      "Epoch: 846, Train Loss: 0.0288\n",
      "Epoch 846 - Train Loss: 0.0288 - Val Loss: 0.0167\n",
      "Epoch: 847, Train Loss: 0.0295\n",
      "Epoch 847 - Train Loss: 0.0295 - Val Loss: 0.0193\n",
      "Epoch: 848, Train Loss: 0.0316\n",
      "Epoch 848 - Train Loss: 0.0316 - Val Loss: 0.0512\n",
      "Epoch: 849, Train Loss: 0.0270\n",
      "Epoch 849 - Train Loss: 0.0270 - Val Loss: 0.0091\n",
      "Epoch: 850, Train Loss: 0.0258\n",
      "Epoch 850 - Train Loss: 0.0258 - Val Loss: 0.0130\n",
      "Epoch: 851, Train Loss: 0.0285\n",
      "Epoch 851 - Train Loss: 0.0285 - Val Loss: 0.0295\n",
      "Epoch: 852, Train Loss: 0.0336\n",
      "Epoch 852 - Train Loss: 0.0336 - Val Loss: 0.0316\n",
      "Epoch: 853, Train Loss: 0.0319\n",
      "Epoch 853 - Train Loss: 0.0319 - Val Loss: 0.0215\n",
      "Epoch: 854, Train Loss: 0.0300\n",
      "Epoch 854 - Train Loss: 0.0300 - Val Loss: 0.0271\n",
      "Epoch: 855, Train Loss: 0.0344\n",
      "Epoch 855 - Train Loss: 0.0344 - Val Loss: 0.0171\n",
      "Epoch: 856, Train Loss: 0.0285\n",
      "Epoch 856 - Train Loss: 0.0285 - Val Loss: 0.0392\n",
      "Epoch: 857, Train Loss: 0.0347\n",
      "Epoch 857 - Train Loss: 0.0347 - Val Loss: 0.0460\n",
      "Epoch: 858, Train Loss: 0.0275\n",
      "Epoch 858 - Train Loss: 0.0275 - Val Loss: 0.0379\n",
      "Epoch: 859, Train Loss: 0.0329\n",
      "Epoch 859 - Train Loss: 0.0329 - Val Loss: 0.0420\n",
      "Epoch: 860, Train Loss: 0.0264\n",
      "Epoch 860 - Train Loss: 0.0264 - Val Loss: 0.0172\n",
      "Epoch: 861, Train Loss: 0.0266\n",
      "Epoch 861 - Train Loss: 0.0266 - Val Loss: 0.0259\n",
      "Epoch: 862, Train Loss: 0.0222\n",
      "Epoch 862 - Train Loss: 0.0222 - Val Loss: 0.0271\n",
      "Epoch: 863, Train Loss: 0.0261\n",
      "Epoch 863 - Train Loss: 0.0261 - Val Loss: 0.0315\n",
      "Epoch: 864, Train Loss: 0.0310\n",
      "Epoch 864 - Train Loss: 0.0310 - Val Loss: 0.0166\n",
      "Epoch: 865, Train Loss: 0.0215\n",
      "Epoch 865 - Train Loss: 0.0215 - Val Loss: 0.0338\n",
      "Epoch: 866, Train Loss: 0.0317\n",
      "Epoch 866 - Train Loss: 0.0317 - Val Loss: 0.0564\n",
      "Epoch: 867, Train Loss: 0.0207\n",
      "Epoch 867 - Train Loss: 0.0207 - Val Loss: 0.0150\n",
      "Epoch: 868, Train Loss: 0.0261\n",
      "Epoch 868 - Train Loss: 0.0261 - Val Loss: 0.0234\n",
      "Epoch: 869, Train Loss: 0.0371\n",
      "Epoch 869 - Train Loss: 0.0371 - Val Loss: 0.0257\n",
      "Epoch: 870, Train Loss: 0.0265\n",
      "Epoch 870 - Train Loss: 0.0265 - Val Loss: 0.0326\n",
      "Epoch: 871, Train Loss: 0.0269\n",
      "Epoch 871 - Train Loss: 0.0269 - Val Loss: 0.0505\n",
      "Epoch: 872, Train Loss: 0.0282\n",
      "Epoch 872 - Train Loss: 0.0282 - Val Loss: 0.0185\n",
      "Epoch: 873, Train Loss: 0.0301\n",
      "Epoch 873 - Train Loss: 0.0301 - Val Loss: 0.0392\n",
      "Epoch: 874, Train Loss: 0.0301\n",
      "Epoch 874 - Train Loss: 0.0301 - Val Loss: 0.0376\n",
      "Epoch: 875, Train Loss: 0.0291\n",
      "Epoch 875 - Train Loss: 0.0291 - Val Loss: 0.0268\n",
      "Epoch: 876, Train Loss: 0.0227\n",
      "Epoch 876 - Train Loss: 0.0227 - Val Loss: 0.0349\n",
      "Epoch: 877, Train Loss: 0.0274\n",
      "Epoch 877 - Train Loss: 0.0274 - Val Loss: 0.0439\n",
      "Epoch: 878, Train Loss: 0.0279\n",
      "Epoch 878 - Train Loss: 0.0279 - Val Loss: 0.0309\n",
      "Epoch: 879, Train Loss: 0.0258\n",
      "Epoch 879 - Train Loss: 0.0258 - Val Loss: 0.0398\n",
      "Epoch: 880, Train Loss: 0.0304\n",
      "Epoch 880 - Train Loss: 0.0304 - Val Loss: 0.0205\n",
      "Epoch: 881, Train Loss: 0.0251\n",
      "Epoch 881 - Train Loss: 0.0251 - Val Loss: 0.0117\n",
      "Epoch: 882, Train Loss: 0.0275\n",
      "Epoch 882 - Train Loss: 0.0275 - Val Loss: 0.0210\n",
      "Epoch: 883, Train Loss: 0.0234\n",
      "Epoch 883 - Train Loss: 0.0234 - Val Loss: 0.0179\n",
      "Epoch: 884, Train Loss: 0.0312\n",
      "Epoch 884 - Train Loss: 0.0312 - Val Loss: 0.0334\n",
      "Epoch: 885, Train Loss: 0.0346\n",
      "Epoch 885 - Train Loss: 0.0346 - Val Loss: 0.0094\n",
      "Epoch: 886, Train Loss: 0.0287\n",
      "Epoch 886 - Train Loss: 0.0287 - Val Loss: 0.0175\n",
      "Epoch: 887, Train Loss: 0.0304\n",
      "Epoch 887 - Train Loss: 0.0304 - Val Loss: 0.0105\n",
      "Epoch: 888, Train Loss: 0.0309\n",
      "Epoch 888 - Train Loss: 0.0309 - Val Loss: 0.0380\n",
      "Epoch: 889, Train Loss: 0.0204\n",
      "Epoch 889 - Train Loss: 0.0204 - Val Loss: 0.0297\n",
      "Epoch: 890, Train Loss: 0.0416\n",
      "Epoch 890 - Train Loss: 0.0416 - Val Loss: 0.0854\n",
      "Epoch: 891, Train Loss: 0.0269\n",
      "Epoch 891 - Train Loss: 0.0269 - Val Loss: 0.0155\n",
      "Epoch: 892, Train Loss: 0.0271\n",
      "Epoch 892 - Train Loss: 0.0271 - Val Loss: 0.0221\n",
      "Epoch: 893, Train Loss: 0.0233\n",
      "Epoch 893 - Train Loss: 0.0233 - Val Loss: 0.0227\n",
      "Epoch: 894, Train Loss: 0.0229\n",
      "Epoch 894 - Train Loss: 0.0229 - Val Loss: 0.0304\n",
      "Epoch: 895, Train Loss: 0.0259\n",
      "Epoch 895 - Train Loss: 0.0259 - Val Loss: 0.0345\n",
      "Epoch: 896, Train Loss: 0.0302\n",
      "Epoch 896 - Train Loss: 0.0302 - Val Loss: 0.0372\n",
      "Epoch: 897, Train Loss: 0.0332\n",
      "Epoch 897 - Train Loss: 0.0332 - Val Loss: 0.0300\n",
      "Epoch: 898, Train Loss: 0.0251\n",
      "Epoch 898 - Train Loss: 0.0251 - Val Loss: 0.0283\n",
      "Epoch: 899, Train Loss: 0.0344\n",
      "Epoch 899 - Train Loss: 0.0344 - Val Loss: 0.0290\n",
      "Epoch: 900, Train Loss: 0.0303\n",
      "Epoch 900 - Train Loss: 0.0303 - Val Loss: 0.0402\n",
      "Epoch: 901, Train Loss: 0.0342\n",
      "Epoch 901 - Train Loss: 0.0342 - Val Loss: 0.0126\n",
      "Epoch: 902, Train Loss: 0.0305\n",
      "Epoch 902 - Train Loss: 0.0305 - Val Loss: 0.0365\n",
      "Epoch: 903, Train Loss: 0.0323\n",
      "Epoch 903 - Train Loss: 0.0323 - Val Loss: 0.0124\n",
      "Epoch: 904, Train Loss: 0.0237\n",
      "Epoch 904 - Train Loss: 0.0237 - Val Loss: 0.0314\n",
      "Epoch: 905, Train Loss: 0.0257\n",
      "Epoch 905 - Train Loss: 0.0257 - Val Loss: 0.0275\n",
      "Epoch: 906, Train Loss: 0.0196\n",
      "Epoch 906 - Train Loss: 0.0196 - Val Loss: 0.0084\n",
      "Epoch: 907, Train Loss: 0.0276\n",
      "Epoch 907 - Train Loss: 0.0276 - Val Loss: 0.0405\n",
      "Epoch: 908, Train Loss: 0.0208\n",
      "Epoch 908 - Train Loss: 0.0208 - Val Loss: 0.0212\n",
      "Epoch: 909, Train Loss: 0.0294\n",
      "Epoch 909 - Train Loss: 0.0294 - Val Loss: 0.0129\n",
      "Epoch: 910, Train Loss: 0.0261\n",
      "Epoch 910 - Train Loss: 0.0261 - Val Loss: 0.0151\n",
      "Epoch: 911, Train Loss: 0.0312\n",
      "Epoch 911 - Train Loss: 0.0312 - Val Loss: 0.0590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 912, Train Loss: 0.0280\n",
      "Epoch 912 - Train Loss: 0.0280 - Val Loss: 0.0270\n",
      "Epoch: 913, Train Loss: 0.0248\n",
      "Epoch 913 - Train Loss: 0.0248 - Val Loss: 0.0238\n",
      "Epoch: 914, Train Loss: 0.0239\n",
      "Epoch 914 - Train Loss: 0.0239 - Val Loss: 0.0061\n",
      "Epoch: 915, Train Loss: 0.0227\n",
      "Epoch 915 - Train Loss: 0.0227 - Val Loss: 0.0230\n",
      "Epoch: 916, Train Loss: 0.0296\n",
      "Epoch 916 - Train Loss: 0.0296 - Val Loss: 0.0458\n",
      "Epoch: 917, Train Loss: 0.0232\n",
      "Epoch 917 - Train Loss: 0.0232 - Val Loss: 0.0392\n",
      "Epoch: 918, Train Loss: 0.0264\n",
      "Epoch 918 - Train Loss: 0.0264 - Val Loss: 0.0318\n",
      "Epoch: 919, Train Loss: 0.0213\n",
      "Epoch 919 - Train Loss: 0.0213 - Val Loss: 0.0083\n",
      "Epoch: 920, Train Loss: 0.0296\n",
      "Epoch 920 - Train Loss: 0.0296 - Val Loss: 0.0181\n",
      "Epoch: 921, Train Loss: 0.0265\n",
      "Epoch 921 - Train Loss: 0.0265 - Val Loss: 0.0250\n",
      "Epoch: 922, Train Loss: 0.0250\n",
      "Epoch 922 - Train Loss: 0.0250 - Val Loss: 0.0360\n",
      "Epoch: 923, Train Loss: 0.0244\n",
      "Epoch 923 - Train Loss: 0.0244 - Val Loss: 0.0213\n",
      "Epoch: 924, Train Loss: 0.0245\n",
      "Epoch 924 - Train Loss: 0.0245 - Val Loss: 0.0257\n",
      "Epoch: 925, Train Loss: 0.0251\n",
      "Epoch 925 - Train Loss: 0.0251 - Val Loss: 0.0353\n",
      "Epoch: 926, Train Loss: 0.0232\n",
      "Epoch 926 - Train Loss: 0.0232 - Val Loss: 0.0099\n",
      "Epoch: 927, Train Loss: 0.0323\n",
      "Epoch 927 - Train Loss: 0.0323 - Val Loss: 0.0206\n",
      "Epoch: 928, Train Loss: 0.0313\n",
      "Epoch 928 - Train Loss: 0.0313 - Val Loss: 0.0283\n",
      "Epoch: 929, Train Loss: 0.0261\n",
      "Epoch 929 - Train Loss: 0.0261 - Val Loss: 0.0276\n",
      "Epoch: 930, Train Loss: 0.0265\n",
      "Epoch 930 - Train Loss: 0.0265 - Val Loss: 0.0120\n",
      "Epoch: 931, Train Loss: 0.0303\n",
      "Epoch 931 - Train Loss: 0.0303 - Val Loss: 0.0203\n",
      "Epoch: 932, Train Loss: 0.0240\n",
      "Epoch 932 - Train Loss: 0.0240 - Val Loss: 0.0117\n",
      "Epoch: 933, Train Loss: 0.0274\n",
      "Epoch 933 - Train Loss: 0.0274 - Val Loss: 0.0140\n",
      "Epoch: 934, Train Loss: 0.0282\n",
      "Epoch 934 - Train Loss: 0.0282 - Val Loss: 0.0246\n",
      "Epoch: 935, Train Loss: 0.0274\n",
      "Epoch 935 - Train Loss: 0.0274 - Val Loss: 0.0219\n",
      "Epoch: 936, Train Loss: 0.0209\n",
      "Epoch 936 - Train Loss: 0.0209 - Val Loss: 0.0353\n",
      "Epoch: 937, Train Loss: 0.0258\n",
      "Epoch 937 - Train Loss: 0.0258 - Val Loss: 0.0332\n",
      "Epoch: 938, Train Loss: 0.0233\n",
      "Epoch 938 - Train Loss: 0.0233 - Val Loss: 0.0403\n",
      "Epoch: 939, Train Loss: 0.0290\n",
      "Epoch 939 - Train Loss: 0.0290 - Val Loss: 0.0142\n",
      "Epoch: 940, Train Loss: 0.0246\n",
      "Epoch 940 - Train Loss: 0.0246 - Val Loss: 0.0153\n",
      "Epoch: 941, Train Loss: 0.0333\n",
      "Epoch 941 - Train Loss: 0.0333 - Val Loss: 0.0371\n",
      "Epoch: 942, Train Loss: 0.0306\n",
      "Epoch 942 - Train Loss: 0.0306 - Val Loss: 0.0397\n",
      "Epoch: 943, Train Loss: 0.0197\n",
      "Epoch 943 - Train Loss: 0.0197 - Val Loss: 0.0261\n",
      "Epoch: 944, Train Loss: 0.0235\n",
      "Epoch 944 - Train Loss: 0.0235 - Val Loss: 0.0155\n",
      "Epoch: 945, Train Loss: 0.0392\n",
      "Epoch 945 - Train Loss: 0.0392 - Val Loss: 0.0089\n",
      "Epoch: 946, Train Loss: 0.0235\n",
      "Epoch 946 - Train Loss: 0.0235 - Val Loss: 0.0251\n",
      "Epoch: 947, Train Loss: 0.0242\n",
      "Epoch 947 - Train Loss: 0.0242 - Val Loss: 0.0135\n",
      "Epoch: 948, Train Loss: 0.0220\n",
      "Epoch 948 - Train Loss: 0.0220 - Val Loss: 0.0136\n",
      "Epoch: 949, Train Loss: 0.0263\n",
      "Epoch 949 - Train Loss: 0.0263 - Val Loss: 0.0218\n",
      "Epoch: 950, Train Loss: 0.0281\n",
      "Epoch 950 - Train Loss: 0.0281 - Val Loss: 0.0097\n",
      "Epoch: 951, Train Loss: 0.0291\n",
      "Epoch 951 - Train Loss: 0.0291 - Val Loss: 0.0490\n",
      "Epoch: 952, Train Loss: 0.0218\n",
      "Epoch 952 - Train Loss: 0.0218 - Val Loss: 0.0197\n",
      "Epoch: 953, Train Loss: 0.0244\n",
      "Epoch 953 - Train Loss: 0.0244 - Val Loss: 0.0096\n",
      "Epoch: 954, Train Loss: 0.0269\n",
      "Epoch 954 - Train Loss: 0.0269 - Val Loss: 0.0211\n",
      "Epoch: 955, Train Loss: 0.0245\n",
      "Epoch 955 - Train Loss: 0.0245 - Val Loss: 0.0215\n",
      "Epoch: 956, Train Loss: 0.0236\n",
      "Epoch 956 - Train Loss: 0.0236 - Val Loss: 0.0151\n",
      "Epoch: 957, Train Loss: 0.0230\n",
      "Epoch 957 - Train Loss: 0.0230 - Val Loss: 0.0185\n",
      "Epoch: 958, Train Loss: 0.0216\n",
      "Epoch 958 - Train Loss: 0.0216 - Val Loss: 0.0121\n",
      "Epoch: 959, Train Loss: 0.0225\n",
      "Epoch 959 - Train Loss: 0.0225 - Val Loss: 0.0291\n",
      "Epoch: 960, Train Loss: 0.0303\n",
      "Epoch 960 - Train Loss: 0.0303 - Val Loss: 0.0116\n",
      "Epoch: 961, Train Loss: 0.0229\n",
      "Epoch 961 - Train Loss: 0.0229 - Val Loss: 0.0340\n",
      "Epoch: 962, Train Loss: 0.0282\n",
      "Epoch 962 - Train Loss: 0.0282 - Val Loss: 0.0367\n",
      "Epoch: 963, Train Loss: 0.0288\n",
      "Epoch 963 - Train Loss: 0.0288 - Val Loss: 0.0100\n",
      "Epoch: 964, Train Loss: 0.0273\n",
      "Epoch 964 - Train Loss: 0.0273 - Val Loss: 0.0096\n",
      "Epoch: 965, Train Loss: 0.0255\n",
      "Epoch 965 - Train Loss: 0.0255 - Val Loss: 0.0191\n",
      "Epoch: 966, Train Loss: 0.0256\n",
      "Epoch 966 - Train Loss: 0.0256 - Val Loss: 0.0276\n",
      "Epoch: 967, Train Loss: 0.0284\n",
      "Epoch 967 - Train Loss: 0.0284 - Val Loss: 0.0222\n",
      "Epoch: 968, Train Loss: 0.0216\n",
      "Epoch 968 - Train Loss: 0.0216 - Val Loss: 0.0160\n",
      "Epoch: 969, Train Loss: 0.0262\n",
      "Epoch 969 - Train Loss: 0.0262 - Val Loss: 0.0137\n",
      "Epoch: 970, Train Loss: 0.0234\n",
      "Epoch 970 - Train Loss: 0.0234 - Val Loss: 0.0353\n",
      "Epoch: 971, Train Loss: 0.0326\n",
      "Epoch 971 - Train Loss: 0.0326 - Val Loss: 0.0387\n",
      "Epoch: 972, Train Loss: 0.0285\n",
      "Epoch 972 - Train Loss: 0.0285 - Val Loss: 0.0229\n",
      "Epoch: 973, Train Loss: 0.0269\n",
      "Epoch 973 - Train Loss: 0.0269 - Val Loss: 0.0156\n",
      "Epoch: 974, Train Loss: 0.0260\n",
      "Epoch 974 - Train Loss: 0.0260 - Val Loss: 0.0088\n",
      "Epoch: 975, Train Loss: 0.0251\n",
      "Epoch 975 - Train Loss: 0.0251 - Val Loss: 0.0213\n",
      "Epoch: 976, Train Loss: 0.0276\n",
      "Epoch 976 - Train Loss: 0.0276 - Val Loss: 0.0429\n",
      "Epoch: 977, Train Loss: 0.0212\n",
      "Epoch 977 - Train Loss: 0.0212 - Val Loss: 0.0305\n",
      "Epoch: 978, Train Loss: 0.0298\n",
      "Epoch 978 - Train Loss: 0.0298 - Val Loss: 0.0267\n",
      "Epoch: 979, Train Loss: 0.0265\n",
      "Epoch 979 - Train Loss: 0.0265 - Val Loss: 0.0279\n",
      "Epoch: 980, Train Loss: 0.0231\n",
      "Epoch 980 - Train Loss: 0.0231 - Val Loss: 0.0147\n",
      "Epoch: 981, Train Loss: 0.0238\n",
      "Epoch 981 - Train Loss: 0.0238 - Val Loss: 0.0078\n",
      "Epoch: 982, Train Loss: 0.0250\n",
      "Epoch 982 - Train Loss: 0.0250 - Val Loss: 0.0306\n",
      "Epoch: 983, Train Loss: 0.0302\n",
      "Epoch 983 - Train Loss: 0.0302 - Val Loss: 0.0210\n",
      "Epoch: 984, Train Loss: 0.0275\n",
      "Epoch 984 - Train Loss: 0.0275 - Val Loss: 0.0110\n",
      "Epoch: 985, Train Loss: 0.0315\n",
      "Epoch 985 - Train Loss: 0.0315 - Val Loss: 0.0487\n",
      "Epoch: 986, Train Loss: 0.0224\n",
      "Epoch 986 - Train Loss: 0.0224 - Val Loss: 0.0184\n",
      "Epoch: 987, Train Loss: 0.0275\n",
      "Epoch 987 - Train Loss: 0.0275 - Val Loss: 0.0342\n",
      "Epoch: 988, Train Loss: 0.0278\n",
      "Epoch 988 - Train Loss: 0.0278 - Val Loss: 0.0228\n",
      "Epoch: 989, Train Loss: 0.0226\n",
      "Epoch 989 - Train Loss: 0.0226 - Val Loss: 0.0179\n",
      "Epoch: 990, Train Loss: 0.0273\n",
      "Epoch 990 - Train Loss: 0.0273 - Val Loss: 0.0349\n",
      "Epoch: 991, Train Loss: 0.0292\n",
      "Epoch 991 - Train Loss: 0.0292 - Val Loss: 0.0078\n",
      "Epoch: 992, Train Loss: 0.0213\n",
      "Epoch 992 - Train Loss: 0.0213 - Val Loss: 0.0199\n",
      "Epoch: 993, Train Loss: 0.0281\n",
      "Epoch 993 - Train Loss: 0.0281 - Val Loss: 0.0098\n",
      "Epoch: 994, Train Loss: 0.0251\n",
      "Epoch 994 - Train Loss: 0.0251 - Val Loss: 0.0146\n",
      "Epoch: 995, Train Loss: 0.0316\n",
      "Epoch 995 - Train Loss: 0.0316 - Val Loss: 0.0399\n",
      "Epoch: 996, Train Loss: 0.0289\n",
      "Epoch 996 - Train Loss: 0.0289 - Val Loss: 0.0265\n",
      "Epoch: 997, Train Loss: 0.0321\n",
      "Epoch 997 - Train Loss: 0.0321 - Val Loss: 0.0165\n",
      "Epoch: 998, Train Loss: 0.0235\n",
      "Epoch 998 - Train Loss: 0.0235 - Val Loss: 0.0198\n",
      "Epoch: 999, Train Loss: 0.0289\n",
      "Epoch 999 - Train Loss: 0.0289 - Val Loss: 0.0146\n",
      "Epoch: 1000, Train Loss: 0.0303\n",
      "Epoch 1000 - Train Loss: 0.0303 - Val Loss: 0.0242\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#data = boston_housing.load_data()#fetch_california_housing()#load_boston()\n",
    "#data = fetch_openml('mnist_784', version=1)\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('./housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "#data = load_boston()\n",
    "#print(data)\n",
    "\n",
    "X_ = data.values\n",
    "y_ = data['MEDV'].values\n",
    "\n",
    "#X_ = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])#= data['data']\n",
    "#y_ = raw_df.values[1::2, 2]#= data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "X_begin = X_\n",
    "y_begin = y_\n",
    "\n",
    "X_, X_test, y_, y_test = train_test_split(\n",
    "    X_begin, y_begin, test_size=0.8, shuffle=False\n",
    ")\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "val_feed_dict = {\n",
    "    X: X_test,\n",
    "    y: y_test,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "history = dict.fromkeys(['val_loss', 'loss'])\n",
    "\n",
    "epochs = 1000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "val_graph = topological_sort(val_feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "loss_arr = []\n",
    "val_loss_arr = []\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "        \n",
    "    loss_arr.append(loss/steps_per_epoch)\n",
    "    \n",
    "    print(\"Epoch: {}, Train Loss: {:.4f}\".format(i+1, loss/steps_per_epoch))\n",
    "    \n",
    "    for node in val_graph:\n",
    "        node.forward()\n",
    "    val_loss = val_graph[-1].value\n",
    "    val_loss_arr.append(val_loss)\n",
    "    print(f\"Epoch {i+1} - Train Loss: {loss/steps_per_epoch:.4f} - Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ba4cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history[\"loss\"] = loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10898587",
   "metadata": {},
   "outputs": [],
   "source": [
    "history[\"val_loss\"] = val_loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "045c0c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bdbac42100>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[\"loss\"][:20],label='Train Loss')\n",
    "plt.plot(history[\"val_loss\"][:20],label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d992963b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bdbacf5ca0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter([i for i in range (20)], history[\"loss\"][:20],label='Train Loss')\n",
    "plt.scatter([i for i in range (20)], history[\"val_loss\"][:20],label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb9f0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = MSE(y, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d196bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.02423078944241056)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost.forward()\n",
    "cost.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c8fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
